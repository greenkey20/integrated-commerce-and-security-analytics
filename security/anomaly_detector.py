# core/anomaly_detection.py
import pandas as pd
import numpy as np
import tensorflow as tf
import keras
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
import joblib
import json
from datetime import datetime, timedelta
import re
from typing import Dict, List, Tuple
import logging

# 07-2.ipynb ìŠ¤íƒ€ì¼ë¡œ ëœë¤ ì‹œë“œ ì„¤ì •
keras.utils.set_random_seed(42)
tf.config.experimental.enable_op_determinism()

class APILogAnomalyDetector:
    """API ë¡œê·¸ ì´ìƒ íƒì§€ ëª¨ë¸ (MLP ê¸°ë°˜)"""
    
    def __init__(self, model_type: str = 'mlp'):
        self.model_type = model_type
        self.mlp_model = None
        self.cnn_model = None
        self.ensemble_model = None
        self.scaler = StandardScaler()
        self.label_encoders = {}
        self.feature_names = []
        self.is_trained = False
        self.sequence_length = 10  # CNNìš© ì‹œí€€ìŠ¤ ê¸¸ì´
        
    def extract_features_from_log(self, log_entry: Dict) -> np.ndarray:
        """ë¡œê·¸ ì—”íŠ¸ë¦¬ì—ì„œ íŠ¹ì„± ì¶”ì¶œ"""
        features = {}
        
        # ì‹œê°„ ê¸°ë°˜ íŠ¹ì„±
        timestamp = datetime.fromisoformat(log_entry.get('timestamp', datetime.now().isoformat()))
        features['hour'] = timestamp.hour
        features['day_of_week'] = timestamp.weekday()
        features['is_weekend'] = 1 if timestamp.weekday() >= 5 else 0
        features['is_business_hour'] = 1 if 9 <= timestamp.hour <= 17 else 0
        
        # ìš”ì²­ ë¹ˆë„ íŠ¹ì„±
        features['requests_per_minute'] = log_entry.get('requests_per_minute', 0)
        
        # ìš”ì²­ í¬ê¸° íŠ¹ì„±
        features['request_size'] = log_entry.get('request_size', 0)
        features['content_length'] = int(log_entry.get('content_length', 0))
        
        # HTTP ë©”ì„œë“œ ì›í•« ì¸ì½”ë”©
        method = log_entry.get('method', 'GET')
        for m in ['GET', 'POST', 'PUT', 'DELETE', 'PATCH']:
            features[f'method_{m}'] = 1 if method == m else 0
        
        # User-Agent ë¶„ì„
        user_agent = log_entry.get('user_agent', '').lower()
        features['ua_length'] = len(user_agent)
        features['ua_has_bot'] = 1 if any(bot in user_agent for bot in ['bot', 'crawler', 'spider']) else 0
        features['ua_has_browser'] = 1 if any(browser in user_agent for browser in ['mozilla', 'chrome', 'safari', 'firefox']) else 0
        features['ua_suspicious'] = 1 if any(tool in user_agent for tool in ['sqlmap', 'nikto', 'nmap', 'curl', 'python']) else 0
        
        # URL íŒ¨í„´ ë¶„ì„
        url = log_entry.get('url', '')
        features['url_length'] = len(url)
        features['url_params_count'] = url.count('&') + (1 if '?' in url else 0)
        features['url_has_sql_keywords'] = 1 if any(keyword in url.lower() for keyword in ['select', 'union', 'drop', 'insert']) else 0
        features['url_has_xss_patterns'] = 1 if any(pattern in url.lower() for pattern in ['<script', 'javascript:', 'alert(']) else 0
        
        # IP ê¸°ë°˜ íŠ¹ì„± (ê°„ë‹¨í™”)
        client_ip = log_entry.get('client_ip', '127.0.0.1')
        ip_parts = client_ip.split('.')
        if len(ip_parts) == 4:
            try:
                features['ip_first_octet'] = int(ip_parts[0])
                features['ip_is_private'] = 1 if ip_parts[0] in ['10', '172', '192'] else 0
            except:
                features['ip_first_octet'] = 0
                features['ip_is_private'] = 0
        else:
            features['ip_first_octet'] = 0
            features['ip_is_private'] = 0
        
        # ì‘ë‹µ ì‹œê°„ (ìˆëŠ” ê²½ìš°)
        features['response_time'] = log_entry.get('processing_time', 0)
        
        return np.array(list(features.values()))
    
    def prepare_training_data(self, log_file_path: str = None, cicids_data: pd.DataFrame = None) -> Tuple[np.ndarray, np.ndarray]:
        """í›ˆë ¨ ë°ì´í„° ì¤€ë¹„"""
        
        if cicids_data is not None:
            # CICIDS2017 ë°ì´í„° í™œìš©
            X, y = self.prepare_cicids_data(cicids_data)
        else:
            # ìì²´ ë¡œê·¸ íŒŒì¼ í™œìš©
            X, y = self.prepare_log_data(log_file_path)
        
        return X, y
    
    def prepare_cicids_data(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """CICIDS2017 ë°ì´í„° ì „ì²˜ë¦¬"""
        # CICIDS2017ì˜ ì£¼ìš” íŠ¹ì„± ì„ íƒ
        feature_columns = [
            'Flow Duration', 'Total Fwd Packets', 'Total Backward Packets',
            'Total Length of Fwd Packets', 'Total Length of Bwd Packets',
            'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Fwd Packet Length Mean',
            'Bwd Packet Length Max', 'Bwd Packet Length Min', 'Bwd Packet Length Mean',
            'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Std',
            'Fwd IAT Total', 'Fwd IAT Mean', 'Bwd IAT Total', 'Bwd IAT Mean'
        ]
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
        available_columns = [col for col in feature_columns if col in df.columns]
        X = df[available_columns].fillna(0)
        
        # ë¼ë²¨ ì²˜ë¦¬ (BENIGN = 0, ê³µê²© = 1)
        y = (df['Label'] != 'BENIGN').astype(int)
        
        # ë¬´í•œëŒ€ ê°’ ì²˜ë¦¬
        X = X.replace([np.inf, -np.inf], 0)
        
        return X.values, y.values
    
    def prepare_log_data(self, log_file_path: str) -> Tuple[np.ndarray, np.ndarray]:
        """ìì²´ ë¡œê·¸ ë°ì´í„° ì „ì²˜ë¦¬"""
        logs = []
        labels = []
        
        with open(log_file_path, 'r') as f:
            for line in f:
                try:
                    log_entry = json.loads(line.strip())
                    features = self.extract_features_from_log(log_entry)
                    
                    # ë¼ë²¨ ê²°ì • (íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜)
                    is_suspicious = log_entry.get('is_suspicious', False)
                    
                    logs.append(features)
                    labels.append(1 if is_suspicious else 0)
                except:
                    continue
        
        return np.array(logs), np.array(labels)
    
    def build_mlp_model(self, input_shape: int) -> keras.Model:
        """07-2.ipynb ìŠ¤íƒ€ì¼ì˜ MLP ëª¨ë¸ êµ¬ì¶•"""
        
        model = keras.Sequential([
            keras.layers.Input(shape=(input_shape,)),
            keras.layers.Dense(128, activation='relu', name='hidden_layer_1'),
            keras.layers.Dropout(0.3),
            keras.layers.Dense(64, activation='relu', name='hidden_layer_2'),
            keras.layers.Dropout(0.2),
            keras.layers.Dense(32, activation='relu', name='hidden_layer_3'),
            keras.layers.Dense(1, activation='sigmoid', name='output_layer')
        ], name='MLP_Anomaly_Detector')
        
        model.compile(
            optimizer='adam',
            loss='binary_crossentropy',
            metrics=['accuracy', 'precision', 'recall']
        )
        
        return model
    
    def build_cnn_model(self, input_shape: int) -> keras.Model:
        """08-2.ipynb ìŠ¤íƒ€ì¼ì˜ CNN ëª¨ë¸ êµ¬ì¶• (ì‹œê³„ì—´ íŒ¨í„´ ìš©)"""
        
        model = keras.Sequential([
            keras.layers.Input(shape=(self.sequence_length, input_shape)),
            keras.layers.Conv1D(64, 3, activation='relu'),
            keras.layers.MaxPooling1D(2),
            keras.layers.Conv1D(32, 3, activation='relu'),
            keras.layers.GlobalAveragePooling1D(),
            keras.layers.Dense(50, activation='relu'),
            keras.layers.Dropout(0.3),
            keras.layers.Dense(1, activation='sigmoid')
        ], name='CNN_Anomaly_Detector')
        
        model.compile(
            optimizer='adam',
            loss='binary_crossentropy',
            metrics=['accuracy', 'precision', 'recall']
        )
        
        return model
    
    def build_hybrid_model(self, input_shape: int) -> keras.Model:
        """í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸: MLP + CNN ê²°í•©"""
        
        # MLP ë¸Œëœì¹˜
        mlp_input = keras.layers.Input(shape=(input_shape,), name='mlp_input')
        mlp_dense1 = keras.layers.Dense(64, activation='relu')(mlp_input)
        mlp_dropout1 = keras.layers.Dropout(0.3)(mlp_dense1)
        mlp_dense2 = keras.layers.Dense(32, activation='relu')(mlp_dropout1)
        mlp_output = keras.layers.Dense(16, activation='relu', name='mlp_features')(mlp_dense2)
        
        # CNN ë¸Œëœì¹˜ (ì‹œê³„ì—´)
        cnn_input = keras.layers.Input(shape=(self.sequence_length, input_shape), name='cnn_input')
        cnn_conv1 = keras.layers.Conv1D(32, 3, activation='relu')(cnn_input)
        cnn_pool1 = keras.layers.MaxPooling1D(2)(cnn_conv1)
        cnn_conv2 = keras.layers.Conv1D(16, 3, activation='relu')(cnn_pool1)
        cnn_global = keras.layers.GlobalAveragePooling1D()(cnn_conv2)
        cnn_output = keras.layers.Dense(16, activation='relu', name='cnn_features')(cnn_global)
        
        # íŠ¹ì„± ìœµí•©
        merged = keras.layers.concatenate([mlp_output, cnn_output], name='feature_fusion')
        fusion_dense = keras.layers.Dense(32, activation='relu')(merged)
        fusion_dropout = keras.layers.Dropout(0.2)(fusion_dense)
        final_output = keras.layers.Dense(1, activation='sigmoid', name='hybrid_output')(fusion_dropout)
        
        model = keras.Model(
            inputs=[mlp_input, cnn_input],
            outputs=final_output,
            name='Hybrid_Anomaly_Detector'
        )
        
        model.compile(
            optimizer='adam',
            loss='binary_crossentropy',
            metrics=['accuracy', 'precision', 'recall']
        )
        
        return model
    
    def prepare_sequence_data(self, X: np.ndarray) -> np.ndarray:
        """CNNìš© ì‹œí€€ìŠ¤ ë°ì´í„° ì¤€ë¹„"""
        sequences = []
        for i in range(len(X) - self.sequence_length + 1):
            sequences.append(X[i:i + self.sequence_length])
        return np.array(sequences)
    
    def train(self, X: np.ndarray, y: np.ndarray, validation_split: float = 0.2, epochs: int = 10):
        """í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ í›ˆë ¨"""
        
        # ë°ì´í„° ìŠ¤ì¼€ì¼ë§
        X_scaled = self.scaler.fit_transform(X)
        
        if self.model_type == 'hybrid':
            # í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ í›ˆë ¨
            print("ğŸ”¥ í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ (MLP + CNN) í›ˆë ¨ ì‹œì‘")
            
            # CNNìš© ì‹œí€€ìŠ¤ ë°ì´í„° ì¤€ë¹„
            X_sequence = self.prepare_sequence_data(X_scaled)
            y_sequence = y[self.sequence_length-1:]  # ì‹œí€€ìŠ¤ì— ë§ê²Œ ë¼ë²¨ ì¡°ì •
            
            # MLPìš© ê°œë³„ ë°ì´í„° (ì‹œí€€ìŠ¤ ë§ˆì§€ë§‰ ê°’ë“¤)
            X_individual = X_scaled[self.sequence_length-1:]
            
            # í›ˆë ¨/ê²€ì¦ ë¶„í• 
            split_idx = int(len(X_sequence) * (1 - validation_split))
            
            X_seq_train, X_seq_val = X_sequence[:split_idx], X_sequence[split_idx:]
            X_ind_train, X_ind_val = X_individual[:split_idx], X_individual[split_idx:]
            y_train, y_val = y_sequence[:split_idx], y_sequence[split_idx:]
            
            # í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ êµ¬ì¶•
            self.ensemble_model = self.build_hybrid_model(X.shape[1])
            
            # í›ˆë ¨ ì‹¤í–‰
            history = self.ensemble_model.fit(
                [X_ind_train, X_seq_train], y_train,
                validation_data=([X_ind_val, X_seq_val], y_val),
                epochs=epochs,
                batch_size=32,
                callbacks=[
                    keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),
                    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2)
                ],
                verbose=1
            )
            
        elif self.model_type == 'mlp':
            # MLP ëª¨ë¸ë§Œ í›ˆë ¨
            print("âœ… MLP ëª¨ë¸ í›ˆë ¨ ì‹œì‘")
            X_train, X_val, y_train, y_val = train_test_split(
                X_scaled, y, test_size=validation_split, random_state=42, stratify=y
            )
            
            self.mlp_model = self.build_mlp_model(X.shape[1])
            
            history = self.mlp_model.fit(
                X_train, y_train,
                validation_data=(X_val, y_val),
                epochs=epochs,
                batch_size=32,
                callbacks=[
                    keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
                ],
                verbose=1
            )
        
        elif self.model_type == 'cnn':
            # CNN ëª¨ë¸ë§Œ í›ˆë ¨
            print("ğŸ“Š CNN ëª¨ë¸ í›ˆë ¨ ì‹œì‘")
            X_sequence = self.prepare_sequence_data(X_scaled)
            y_sequence = y[self.sequence_length-1:]
            
            X_train, X_val, y_train, y_val = train_test_split(
                X_sequence, y_sequence, test_size=validation_split, random_state=42, stratify=y_sequence
            )
            
            self.cnn_model = self.build_cnn_model(X.shape[1])
            
            history = self.cnn_model.fit(
                X_train, y_train,
                validation_data=(X_val, y_val),
                epochs=epochs,
                batch_size=32,
                callbacks=[
                    keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
                ],
                verbose=1
            )
        
        self.is_trained = True
        return history
    
    def predict(self, log_entry: Dict) -> Tuple[float, bool]:
        """ë‹¨ì¼ ë¡œê·¸ ì—”íŠ¸ë¦¬ì— ëŒ€í•œ ì˜ˆì¸¡"""
        if not self.is_trained:
            raise ValueError("ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        features = self.extract_features_from_log(log_entry)
        features_scaled = self.scaler.transform([features])
        
        if self.model_type == 'hybrid' and self.ensemble_model:
            # í•˜ì´ë¸Œë¦¬ë“œ ì˜ˆì¸¡ (ë”ë¯¸ ì‹œí€€ìŠ¤ ì‚¬ìš©)
            dummy_sequence = np.repeat(features_scaled, self.sequence_length, axis=0).reshape(1, self.sequence_length, -1)
            probability = self.ensemble_model.predict([features_scaled, dummy_sequence], verbose=0)[0][0]
        elif self.model_type == 'mlp' and self.mlp_model:
            probability = self.mlp_model.predict(features_scaled, verbose=0)[0][0]
        elif self.model_type == 'cnn' and self.cnn_model:
            dummy_sequence = np.repeat(features_scaled, self.sequence_length, axis=0).reshape(1, self.sequence_length, -1)
            probability = self.cnn_model.predict(dummy_sequence, verbose=0)[0][0]
        else:
            # í´ë°±: ëœë¤ ì˜ˆì¸¡
            probability = np.random.uniform(0.1, 0.3)
        
        is_anomaly = probability > 0.5
        return float(probability), is_anomaly
    
    def save_model(self, model_path: str = "models/hybrid_detector"):
        """ëª¨ë¸ ì €ì¥"""
        import os
        os.makedirs(os.path.dirname(model_path), exist_ok=True)
        
        # ê° ëª¨ë¸ë³„ë¡œ ì €ì¥
        if self.ensemble_model:
            self.ensemble_model.save(f"{model_path}_hybrid.keras")
        if self.mlp_model:
            self.mlp_model.save(f"{model_path}_mlp.keras")
        if self.cnn_model:
            self.cnn_model.save(f"{model_path}_cnn.keras")
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
        joblib.dump(self.scaler, f"{model_path}_scaler.pkl")
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata = {
            "model_type": self.model_type,
            "is_trained": self.is_trained,
            "sequence_length": self.sequence_length
        }
        with open(f"{model_path}_metadata.json", 'w') as f:
            json.dump(metadata, f)
    
    def load_model(self, model_path: str = "models/hybrid_detector"):
        """ëª¨ë¸ ë¡œë“œ"""
        try:
            # ë©”íƒ€ë°ì´í„° ë¡œë“œ
            with open(f"{model_path}_metadata.json", 'r') as f:
                metadata = json.load(f)
            
            self.model_type = metadata["model_type"]
            self.is_trained = metadata["is_trained"]
            self.sequence_length = metadata.get("sequence_length", 10)
            
            # ê° ëª¨ë¸ ë¡œë“œ ì‹œë„
            try:
                self.ensemble_model = keras.models.load_model(f"{model_path}_hybrid.keras")
                print("âœ… í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            except:
                pass
            
            try:
                self.mlp_model = keras.models.load_model(f"{model_path}_mlp.keras")
                print("âœ… MLP ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            except:
                pass
            
            try:
                self.cnn_model = keras.models.load_model(f"{model_path}_cnn.keras")
                print("âœ… CNN ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            except:
                pass
            
            # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
            self.scaler = joblib.load(f"{model_path}_scaler.pkl")
            
            print("ğŸ”¥ ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
        except Exception as e:
            print(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")

# ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤
class RealTimeAnomalyMonitor:
    """ì‹¤ì‹œê°„ ì´ìƒ íƒì§€ ëª¨ë‹ˆí„°"""
    
    def __init__(self, detector: APILogAnomalyDetector):
        self.detector = detector
        self.alert_threshold = 0.7
        self.recent_anomalies = []
        self.max_recent_count = 100
        
    def process_log_entry(self, log_entry: Dict) -> Dict:
        """ë¡œê·¸ ì—”íŠ¸ë¦¬ ì‹¤ì‹œê°„ ì²˜ë¦¬"""
        try:
            probability, is_anomaly = self.detector.predict(log_entry)
            
            result = {
                "timestamp": log_entry.get("timestamp"),
                "client_ip": log_entry.get("client_ip"),
                "anomaly_probability": probability,
                "is_anomaly": is_anomaly,
                "alert_level": self.get_alert_level(probability)
            }
            
            # ê³ ìœ„í—˜ ì´ìƒ íƒì§€ ì‹œ ì•Œë¦¼
            if probability > self.alert_threshold:
                self.trigger_alert(log_entry, result)
            
            # ìµœê·¼ ì´ìƒ ê¸°ë¡ ì—…ë°ì´íŠ¸
            if is_anomaly:
                self.recent_anomalies.append(result)
                if len(self.recent_anomalies) > self.max_recent_count:
                    self.recent_anomalies.pop(0)
            
            return result
            
        except Exception as e:
            logging.error(f"ì´ìƒ íƒì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return {"error": str(e)}
    
    def get_alert_level(self, probability: float) -> str:
        """ìœ„í—˜ë„ ë ˆë²¨ ê²°ì •"""
        if probability >= 0.9:
            return "CRITICAL"
        elif probability >= 0.7:
            return "HIGH"
        elif probability >= 0.5:
            return "MEDIUM"
        else:
            return "LOW"
    
    def trigger_alert(self, log_entry: Dict, detection_result: Dict):
        """ì•Œë¦¼ ë°œì†¡"""
        alert_message = {
            "alert_type": "API_ANOMALY_DETECTED",
            "timestamp": datetime.now().isoformat(),
            "client_ip": log_entry.get("client_ip"),
            "anomaly_probability": detection_result["anomaly_probability"],
            "alert_level": detection_result["alert_level"],
            "details": {
                "user_agent": log_entry.get("user_agent"),
                "url": log_entry.get("url"),
                "method": log_entry.get("method"),
                "requests_per_minute": log_entry.get("requests_per_minute")
            }
        }
        
        # ë¡œê·¸ì— ê¸°ë¡
        logging.warning(f"ANOMALY ALERT: {json.dumps(alert_message)}")
    
    def get_statistics(self) -> Dict:
        """ëª¨ë‹ˆí„°ë§ í†µê³„"""
        total_recent = len(self.recent_anomalies)
        if total_recent == 0:
            return {"message": "ìµœê·¼ ì´ìƒ íƒì§€ ì—†ìŒ"}
        
        high_risk_count = sum(1 for a in self.recent_anomalies if a["alert_level"] in ["HIGH", "CRITICAL"])
        avg_probability = np.mean([a["anomaly_probability"] for a in self.recent_anomalies])
        
        return {
            "recent_anomalies_count": total_recent,
            "high_risk_count": high_risk_count,
            "average_anomaly_probability": round(avg_probability, 3),
            "risk_ratio": round(high_risk_count / total_recent, 3) if total_recent > 0 else 0
        }
    
    def get_advanced_statistics(self) -> Dict:
        """ê³ ê¸‰ í†µê³„ (í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œìš©)"""
        stats = self.get_statistics()
        
        # ê³ ìœ„í—˜ IP ì¶”ì¶œ
        high_risk_ips = []
        for anomaly in self.recent_anomalies:
            if anomaly.get("alert_level") in ["HIGH", "CRITICAL"]:
                ip = anomaly.get("client_ip")
                if ip and ip not in high_risk_ips:
                    high_risk_ips.append(ip)
        
        stats.update({
            "total_anomalies": len(self.recent_anomalies),
            "high_risk_ips": high_risk_ips[:10],  # ìµœëŒ€ 10ê°œë§Œ
            "detection_model": self.detector.model_type,
            "model_trained": self.detector.is_trained
        })
        
        return stats

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    detector = APILogAnomalyDetector(model_type='hybrid')
    monitor = RealTimeAnomalyMonitor(detector)
    
    # í…ŒìŠ¤íŠ¸ ë¡œê·¸
    test_log = {
        "timestamp": datetime.now().isoformat(),
        "client_ip": "192.168.1.100",
        "method": "POST",
        "url": "/api/v1/customer/segment",
        "user_agent": "Mozilla/5.0",
        "requests_per_minute": 5,
        "request_size": 256,
        "content_length": 128,
        "processing_time": 0.15
    }
    
    print("í•˜ì´ë¸Œë¦¬ë“œ ì´ìƒ íƒì§€ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
