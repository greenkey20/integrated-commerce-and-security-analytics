#!/usr/bin/env python3
"""
ğŸ§  Customer Segmentation - í†µí•© ëª¨ë¸ í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸
ëª¨ë“  ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ í›ˆë ¨í•˜ê³  ì €ì¥í•˜ëŠ” í†µí•© ìŠ¤í¬ë¦½íŠ¸
"""

import sys
import os
from pathlib import Path
import argparse
import json
import logging
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì„¤ì •
PROJECT_ROOT = Path(__file__).parent.absolute()
sys.path.insert(0, str(PROJECT_ROOT))

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def setup_directories():
    """í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±"""
    directories = [
        PROJECT_ROOT / "models" / "saved_models",
        PROJECT_ROOT / "models" / "checkpoints",
        PROJECT_ROOT / "data" / "processed",
        PROJECT_ROOT / "logs"
    ]
    
    for directory in directories:
        directory.mkdir(parents=True, exist_ok=True)
        logger.info(f"ë””ë ‰í† ë¦¬ ìƒì„±/í™•ì¸: {directory}")

def check_dependencies():
    """í•„ìˆ˜ ì˜ì¡´ì„± í™•ì¸"""
    required_packages = [
        'pandas', 'numpy', 'scikit-learn', 'tensorflow', 
        'plotly', 'streamlit', 'fastapi', 'uvicorn'
    ]
    
    missing_packages = []
    for package in required_packages:
        try:
            __import__(package)
        except ImportError:
            missing_packages.append(package)
    
    if missing_packages:
        logger.error(f"ëˆ„ë½ëœ íŒ¨í‚¤ì§€: {', '.join(missing_packages)}")
        print(f"âŒ ë‹¤ìŒ íŒ¨í‚¤ì§€ë“¤ì„ ì„¤ì¹˜í•´ì£¼ì„¸ìš”: {' '.join(missing_packages)}")
        print(f"pip install {' '.join(missing_packages)}")
        return False
    
    logger.info("âœ… ëª¨ë“  ì˜ì¡´ì„± í™•ì¸ ì™„ë£Œ")
    return True

def load_data():
    """ë°ì´í„° ë¡œë“œ"""
    try:
        import pandas as pd
        
        # ë°ì´í„° íŒŒì¼ ê²½ë¡œ
        data_path = PROJECT_ROOT / "data" / "Mall_Customers.csv"
        
        if not data_path.exists():
            # GitHubì—ì„œ ë°ì´í„° ë‹¤ìš´ë¡œë“œ
            logger.info("ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. GitHubì—ì„œ ë‹¤ìš´ë¡œë“œ ì¤‘...")
            url = "https://raw.githubusercontent.com/tirthajyoti/Machine-Learning-with-Python/master/Datasets/Mall_Customers.csv"
            data = pd.read_csv(url)
            
            # ë°ì´í„° ì €ì¥
            data.to_csv(data_path, index=False)
            logger.info(f"ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {data_path}")
        else:
            data = pd.read_csv(data_path)
            logger.info(f"ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(data)}ê°œ ìƒ˜í”Œ")
        
        return data
    
    except Exception as e:
        logger.error(f"ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

def train_clustering_model(data, n_clusters=5):
    """í´ëŸ¬ìŠ¤í„°ë§ ëª¨ë¸ í›ˆë ¨"""
    try:
        from sklearn.cluster import KMeans
        from sklearn.preprocessing import StandardScaler
        from sklearn.metrics import silhouette_score
        import joblib
        
        logger.info("ğŸ¯ í´ëŸ¬ìŠ¤í„°ë§ ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")
        
        # íŠ¹ì„± ì„ íƒ ë° ì •ê·œí™”
        features = data[["Age", "Annual Income (k$)", "Spending Score (1-100)"]]
        scaler = StandardScaler()
        scaled_features = scaler.fit_transform(features)
        
        # K-means í´ëŸ¬ìŠ¤í„°ë§
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        cluster_labels = kmeans.fit_predict(scaled_features)
        
        # ì„±ëŠ¥ í‰ê°€
        silhouette_avg = silhouette_score(scaled_features, cluster_labels)
        
        # ëª¨ë¸ ì €ì¥
        model_dir = PROJECT_ROOT / "models" / "saved_models"
        joblib.dump(kmeans, model_dir / "kmeans_model.pkl")
        joblib.dump(scaler, model_dir / "scaler.pkl")
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata = {
            "model_type": "kmeans",
            "n_clusters": n_clusters,
            "silhouette_score": float(silhouette_avg),
            "training_date": datetime.now().isoformat(),
            "n_samples": len(data),
            "feature_names": features.columns.tolist()
        }
        
        with open(model_dir / "clustering_metadata.json", "w") as f:
            json.dump(metadata, f, indent=2)
        
        logger.info(f"âœ… í´ëŸ¬ìŠ¤í„°ë§ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ (ì‹¤ë£¨ì—£ ì ìˆ˜: {silhouette_avg:.3f})")
        return True
        
    except Exception as e:
        logger.error(f"í´ëŸ¬ìŠ¤í„°ë§ ëª¨ë¸ í›ˆë ¨ ì‹¤íŒ¨: {e}")
        return False

def train_deep_learning_model(data, n_clusters=5):
    """ë”¥ëŸ¬ë‹ ëª¨ë¸ í›ˆë ¨"""
    try:
        import tensorflow as tf
        from tensorflow import keras
        from sklearn.preprocessing import StandardScaler
        from sklearn.model_selection import train_test_split
        from sklearn.cluster import KMeans
        import joblib
        
        logger.info("ğŸ§  ë”¥ëŸ¬ë‹ ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")
        
        # ë°ì´í„° ì¤€ë¹„
        features = data[["Age", "Annual Income (k$)", "Spending Score (1-100)"]]
        scaler = StandardScaler()
        scaled_features = scaler.fit_transform(features)
        
        # í´ëŸ¬ìŠ¤í„°ë§ìœ¼ë¡œ ë¼ë²¨ ìƒì„±
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        cluster_labels = kmeans.fit_predict(scaled_features)
        
        # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í• 
        X_train, X_test, y_train, y_test = train_test_split(
            scaled_features, cluster_labels, test_size=0.2, 
            random_state=42, stratify=cluster_labels
        )
        
        # ëª¨ë¸ êµ¬ì„±
        model = keras.Sequential([
            keras.layers.Dense(64, activation='relu', input_shape=(3,)),
            keras.layers.Dropout(0.2),
            keras.layers.Dense(32, activation='relu'),
            keras.layers.Dropout(0.1),
            keras.layers.Dense(n_clusters, activation='softmax')
        ])
        
        model.compile(
            optimizer='adam',
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )
        
        # ëª¨ë¸ í›ˆë ¨
        history = model.fit(
            X_train, y_train,
            validation_data=(X_test, y_test),
            epochs=100,
            batch_size=32,
            verbose=0,
            callbacks=[
                keras.callbacks.EarlyStopping(
                    monitor='val_loss', patience=10, restore_best_weights=True
                )
            ]
        )
        
        # ëª¨ë¸ í‰ê°€
        test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)
        
        # ëª¨ë¸ ì €ì¥
        model_dir = PROJECT_ROOT / "models" / "saved_models"
        model.save(model_dir / "deep_learning_model.h5")
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata = {
            "model_type": "deep_learning",
            "n_clusters": n_clusters,
            "test_accuracy": float(test_accuracy),
            "test_loss": float(test_loss),
            "training_date": datetime.now().isoformat(),
            "n_samples": len(data),
            "epochs_trained": len(history.history['loss']),
            "architecture": {
                "layers": ["Dense(64, relu)", "Dropout(0.2)", "Dense(32, relu)", "Dropout(0.1)", f"Dense({n_clusters}, softmax)"],
                "optimizer": "adam",
                "loss": "sparse_categorical_crossentropy"
            }
        }
        
        with open(model_dir / "deep_learning_metadata.json", "w") as f:
            json.dump(metadata, f, indent=2)
        
        logger.info(f"âœ… ë”¥ëŸ¬ë‹ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ (ì •í™•ë„: {test_accuracy:.3f})")
        return True
        
    except Exception as e:
        logger.error(f"ë”¥ëŸ¬ë‹ ëª¨ë¸ í›ˆë ¨ ì‹¤íŒ¨: {e}")
        return False

def create_dummy_models():
    """ë”ë¯¸ ëª¨ë¸ ìƒì„± (API í…ŒìŠ¤íŠ¸ìš©)"""
    try:
        import json
        
        logger.info("ğŸ”§ ë”ë¯¸ ëª¨ë¸ íŒŒì¼ ìƒì„± ì¤‘...")
        
        model_dir = PROJECT_ROOT / "models" / "saved_models"
        
        # ë”ë¯¸ ë©”íƒ€ë°ì´í„°ë“¤
        dummy_files = {
            "hybrid_detector_metadata.json": {
                "model_type": "hybrid",
                "version": "1.0.0",
                "training_date": datetime.now().isoformat(),
                "sequence_length": 10,
                "feature_columns": ["requests_per_minute", "request_size", "content_length", "processing_time"],
                "is_trained": True
            },
            "hybrid_detector_scaler.json": {
                "mean_": [10.5, 1024.0, 512.0, 0.15],
                "scale_": [5.2, 2048.0, 1024.0, 0.08],
                "n_features_in_": 4
            },
            "hybrid_detector_weights.json": {
                "mlp_weights": "dummy_weights_placeholder",
                "cnn_weights": "dummy_weights_placeholder", 
                "ensemble_weights": "dummy_weights_placeholder",
                "note": "ì´ê²ƒì€ í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ ë°ì´í„°ì…ë‹ˆë‹¤."
            }
        }
        
        for filename, content in dummy_files.items():
            with open(model_dir / filename, "w") as f:
                json.dump(content, f, indent=2)
        
        logger.info("âœ… ë”ë¯¸ ëª¨ë¸ íŒŒì¼ ìƒì„± ì™„ë£Œ")
        return True
        
    except Exception as e:
        logger.error(f"ë”ë¯¸ ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
        return False

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="Customer Segmentation ëª¨ë¸ í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸")
    parser.add_argument("--mode", choices=["all", "clustering", "deep_learning", "dummy"], 
                       default="all", help="í›ˆë ¨í•  ëª¨ë¸ íƒ€ì…")
    parser.add_argument("--clusters", type=int, default=5, help="í´ëŸ¬ìŠ¤í„° ê°œìˆ˜")
    parser.add_argument("--skip-deps", action="store_true", help="ì˜ì¡´ì„± ì²´í¬ ê±´ë„ˆë›°ê¸°")
    
    args = parser.parse_args()
    
    print("ğŸ§  Customer Segmentation ëª¨ë¸ í›ˆë ¨ ì‹œì‘")
    print(f"ğŸ“Š í›ˆë ¨ ëª¨ë“œ: {args.mode}")
    print(f"ğŸ¯ í´ëŸ¬ìŠ¤í„° ê°œìˆ˜: {args.clusters}")
    print("-" * 50)
    
    # ì˜ì¡´ì„± í™•ì¸
    if not args.skip_deps and not check_dependencies():
        sys.exit(1)
    
    # ë””ë ‰í† ë¦¬ ì„¤ì •
    setup_directories()
    
    # ë°ì´í„° ë¡œë“œ
    data = load_data()
    if data is None:
        logger.error("ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨")
        sys.exit(1)
    
    # ëª¨ë¸ í›ˆë ¨ ì‹¤í–‰
    success = True
    
    if args.mode in ["all", "clustering"]:
        success &= train_clustering_model(data, args.clusters)
    
    if args.mode in ["all", "deep_learning"]:
        success &= train_deep_learning_model(data, args.clusters)
    
    if args.mode in ["all", "dummy"]:
        success &= create_dummy_models()
    
    # ê²°ê³¼ ì¶œë ¥
    if success:
        print("\n" + "="*50)
        print("ğŸ‰ ëª¨ë¸ í›ˆë ¨ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ!")
        print(f"ğŸ“ ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {PROJECT_ROOT / 'models' / 'saved_models'}")
        print("\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
        print("1. ì›¹ ì•± ì‹¤í–‰: streamlit run main_app.py")
        print("2. API ì„œë²„ ì‹¤í–‰: python api_server.py")
        print("="*50)
    else:
        print("\nâŒ ëª¨ë¸ í›ˆë ¨ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        print("ë¡œê·¸ë¥¼ í™•ì¸í•˜ì—¬ ë¬¸ì œë¥¼ í•´ê²°í•˜ì„¸ìš”.")
        sys.exit(1)

if __name__ == "__main__":
    main()
