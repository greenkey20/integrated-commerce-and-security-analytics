"""
ë”¥ëŸ¬ë‹ ëª¨ë¸ ëª¨ë“ˆ

ë¶„ë¥˜ ëª¨ë¸, ì˜¤í† ì¸ì½”ë” ë“± ë”¥ëŸ¬ë‹ ëª¨ë¸ë“¤ì„ ë‹´ë‹¹
"""

import numpy as np
import pandas as pd
import streamlit as st
import plotly.graph_objects as go
import plotly.express as px
from sklearn.model_selection import train_test_split
from config.settings import DeepLearningConfig

# TensorFlow ë° ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë™ì  ë¡œë”©
try:
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import layers
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Dense, Dropout
    from tensorflow.keras.optimizers import Adam
    from tensorflow.keras.callbacks import EarlyStopping
    TENSORFLOW_AVAILABLE = True
except ImportError:
    TENSORFLOW_AVAILABLE = False


class DeepLearningModels:
    """ë”¥ëŸ¬ë‹ ëª¨ë¸ë“¤ì„ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.classification_model = None
        self.autoencoder_model = None
        self.encoder_model = None
        
    def create_safe_classification_model(self, input_dim, n_clusters, 
                                       hidden_units=None, dropout_rate=None, learning_rate=None):
        """ì•ˆì „í•œ ë¶„ë¥˜ ëª¨ë¸ ìƒì„± í•¨ìˆ˜"""
        
        # ê¸°ë³¸ê°’ ì„¤ì •
        hidden_units = hidden_units or DeepLearningConfig.DEFAULT_HIDDEN_UNITS
        dropout_rate = dropout_rate or DeepLearningConfig.DEFAULT_DROPOUT_RATE
        learning_rate = learning_rate or DeepLearningConfig.DEFAULT_LEARNING_RATE
        
        try:
            # í•µì‹¬ ìˆ˜ì •ì‚¬í•­: ëª¨ë¸ ìƒì„± ì „ í•­ìƒ ì„¸ì…˜ ì´ˆê¸°í™”
            keras.backend.clear_session()

            # ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ìœ„í•œ ì‹œë“œ ì„¤ì •
            tf.keras.utils.set_random_seed(42)

            # ê³ ìœ í•œ íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±ìœ¼ë¡œ ë ˆì´ì–´ ì´ë¦„ ì¶©ëŒ ë°©ì§€
            import time
            timestamp = str(int(time.time() * 1000000))[-8:]  # ë§ˆì´í¬ë¡œì´ˆ ë‹¨ìœ„ 8ìë¦¬

            # Sequential ëª¨ë¸ ìƒì„± - ê° ë ˆì´ì–´ì— ê³ ìœ  ì´ë¦„ ë¶€ì—¬
            model = Sequential([
                Dense(
                    hidden_units,
                    activation="relu",
                    input_shape=(input_dim,),
                    name=f"input_dense_{timestamp}",
                ),
                Dropout(dropout_rate, name=f"dropout_1_{timestamp}"),
                Dense(
                    hidden_units // 2,
                    activation="relu",
                    name=f"hidden_dense_{timestamp}",
                ),
                Dropout(dropout_rate / 2, name=f"dropout_2_{timestamp}"),
                Dense(
                    n_clusters, activation="softmax", name=f"output_dense_{timestamp}"
                ),
            ])

            # ëª¨ë¸ ì»´íŒŒì¼
            model.compile(
                optimizer=Adam(learning_rate=learning_rate),
                loss="sparse_categorical_crossentropy",
                metrics=["accuracy"],
            )

            self.classification_model = model
            return model, None

        except Exception as e:
            return None, f"ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {str(e)}"

    def train_model_with_progress(self, model, X_train, y_train, X_test, y_test, 
                                epochs=None, progress_bar=None, status_text=None):
        """ì§„í–‰ìƒí™©ì„ í‘œì‹œí•˜ë©´ì„œ ëª¨ë¸ì„ í›ˆë ¨í•˜ëŠ” í•¨ìˆ˜"""
        
        epochs = epochs or DeepLearningConfig.DEFAULT_EPOCHS
        
        try:
            # ì¡°ê¸° ì¢…ë£Œ ì½œë°± ì„¤ì •
            early_stopping = EarlyStopping(
                monitor="val_loss", 
                patience=DeepLearningConfig.EARLY_STOPPING_PATIENCE, 
                restore_best_weights=True
            )

            callbacks = [early_stopping]

            # Streamlit ì „ìš© ì½œë°± í´ë˜ìŠ¤ (progress_barê°€ ìˆì„ ë•Œë§Œ)
            if progress_bar and status_text:
                class StreamlitProgressCallback(keras.callbacks.Callback):
                    def __init__(self, total_epochs, progress_bar, status_text):
                        super().__init__()
                        self.total_epochs = total_epochs
                        self.progress_bar = progress_bar
                        self.status_text = status_text

                    def on_epoch_end(self, epoch, logs=None):
                        # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                        progress = (epoch + 1) / self.total_epochs
                        self.progress_bar.progress(progress)

                        # ìƒíƒœ í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
                        if logs:
                            self.status_text.text(
                                f"ì—í¬í¬ {epoch + 1}/{self.total_epochs} - "
                                f"ì†ì‹¤: {logs.get('loss', 0):.4f}, "
                                f"ì •í™•ë„: {logs.get('accuracy', 0):.4f}, "
                                f"ê²€ì¦ ì •í™•ë„: {logs.get('val_accuracy', 0):.4f}"
                            )

                # ì½œë°± ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
                progress_callback = StreamlitProgressCallback(epochs, progress_bar, status_text)
                callbacks.append(progress_callback)

            # ëª¨ë¸ í›ˆë ¨ ì‹¤í–‰
            history = model.fit(
                X_train, y_train,
                validation_data=(X_test, y_test),
                epochs=epochs,
                batch_size=DeepLearningConfig.BATCH_SIZE,
                callbacks=callbacks,
                verbose=0  # ì½˜ì†” ì¶œë ¥ ë¹„í™œì„±í™”
            )

            return history, None

        except Exception as e:
            return None, f"ëª¨ë¸ í›ˆë ¨ ì‹¤íŒ¨: {str(e)}"

    def display_model_architecture_info(self, hidden_units, dropout_rate, n_clusters):
        """ëª¨ë¸ ì•„í‚¤í…ì²˜ ì •ë³´ë¥¼ ì‚¬ìš©ì ì¹œí™”ì ìœ¼ë¡œ í‘œì‹œí•˜ëŠ” í•¨ìˆ˜"""
        st.write("**ğŸ—ï¸ êµ¬ì„±ëœ ì‹ ê²½ë§ êµ¬ì¡°:**")

        architecture_info = [
            f"ì…ë ¥ì¸µ: 3ê°œ íŠ¹ì„± (ë‚˜ì´, ì†Œë“, ì§€ì¶œì ìˆ˜)",
            f"ì€ë‹‰ì¸µ 1: {hidden_units}ê°œ ë‰´ëŸ° + ReLU í™œì„±í™” í•¨ìˆ˜",
            f"ë“œë¡­ì•„ì›ƒ 1: {dropout_rate*100:.0f}% ë‰´ëŸ° ë¬´ì‘ìœ„ ë¹„í™œì„±í™” (ê³¼ì í•© ë°©ì§€)",
            f"ì€ë‹‰ì¸µ 2: {hidden_units//2}ê°œ ë‰´ëŸ° + ReLU í™œì„±í™” í•¨ìˆ˜",
            f"ë“œë¡­ì•„ì›ƒ 2: {dropout_rate/2*100:.0f}% ë‰´ëŸ° ë¬´ì‘ìœ„ ë¹„í™œì„±í™”",
            f"ì¶œë ¥ì¸µ: {n_clusters}ê°œ ë‰´ëŸ° + Softmax (ê° í´ëŸ¬ìŠ¤í„° í™•ë¥  ê³„ì‚°)",
        ]

        for i, layer_info in enumerate(architecture_info, 1):
            st.write(f"{i}. {layer_info}")

    def evaluate_and_display_results(self, model, X_test, y_test, history, n_clusters):
        """ëª¨ë¸ ì„±ëŠ¥ì„ í‰ê°€í•˜ê³  ê²°ê³¼ë¥¼ ì‹œê°í™”í•˜ëŠ” í•¨ìˆ˜"""
        try:
            # ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
            test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)

            # ì˜ˆì¸¡ ìˆ˜í–‰
            y_pred_probs = model.predict(X_test, verbose=0)
            y_pred_classes = np.argmax(y_pred_probs, axis=1)

            # ì˜ˆì¸¡ ì‹ ë¢°ë„ ê³„ì‚°
            confidence_scores = np.max(y_pred_probs, axis=1)
            avg_confidence = np.mean(confidence_scores)

            # ê²°ê³¼ í‘œì‹œ
            st.subheader("ğŸ“ˆ ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„")

            col1, col2 = st.columns(2)

            with col1:
                st.metric("í…ŒìŠ¤íŠ¸ ì •í™•ë„", f"{test_accuracy:.3f}")
                st.metric("í›ˆë ¨ëœ ì—í¬í¬ ìˆ˜", len(history.history["loss"]))
                st.metric("í‰ê·  ì˜ˆì¸¡ ì‹ ë¢°ë„", f"{avg_confidence:.3f}")

            with col2:
                # í›ˆë ¨ ê³¼ì • ì‹œê°í™”
                fig = go.Figure()

                epochs_range = range(1, len(history.history["accuracy"]) + 1)

                fig.add_trace(go.Scatter(
                    x=list(epochs_range),
                    y=history.history["accuracy"],
                    mode="lines",
                    name="í›ˆë ¨ ì •í™•ë„",
                    line=dict(color="blue"),
                ))

                fig.add_trace(go.Scatter(
                    x=list(epochs_range),
                    y=history.history["val_accuracy"],
                    mode="lines",
                    name="ê²€ì¦ ì •í™•ë„",
                    line=dict(color="red"),
                ))

                fig.update_layout(
                    title="ëª¨ë¸ í›ˆë ¨ ê³¼ì •",
                    xaxis_title="ì—í¬í¬",
                    yaxis_title="ì •í™•ë„",
                    height=400,
                )

                st.plotly_chart(fig, use_container_width=True)

            return {
                "test_accuracy": test_accuracy,
                "predictions": y_pred_classes,
                "probabilities": y_pred_probs,
                "confidence": avg_confidence,
            }

        except Exception as e:
            st.error(f"ëª¨ë¸ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return None

    def create_autoencoder(self, input_dim, encoding_dim=None):
        """ì˜¤í† ì¸ì½”ë” ëª¨ë¸ ìƒì„±"""
        encoding_dim = encoding_dim or DeepLearningConfig.DEFAULT_ENCODING_DIM
        
        try:
            # ëª¨ë¸ êµ¬ì„±ì„ ìœ„í•œ ê³ ìœ  ì´ë¦„ ìƒì„±
            import time
            timestamp = str(int(time.time() * 1000))[-6:]

            # ì˜¤í† ì¸ì½”ë” ëª¨ë¸ êµ¬ì„±
            input_layer = layers.Input(shape=(input_dim,), name=f"ae_input_{timestamp}")

            # ì¸ì½”ë”
            encoded = layers.Dense(8, activation="relu", name=f"ae_encode1_{timestamp}")(input_layer)
            encoded = layers.Dense(encoding_dim, activation="relu", name=f"ae_encoded_{timestamp}")(encoded)

            # ë””ì½”ë”
            decoded = layers.Dense(8, activation="relu", name=f"ae_decode1_{timestamp}")(encoded)
            decoded = layers.Dense(input_dim, activation="linear", name=f"ae_output_{timestamp}")(decoded)

            # ëª¨ë¸ ìƒì„±
            autoencoder = keras.Model(input_layer, decoded, name=f"autoencoder_{timestamp}")
            encoder = keras.Model(input_layer, encoded, name=f"encoder_{timestamp}")

            # ì»´íŒŒì¼
            autoencoder.compile(optimizer="adam", loss="mse")

            self.autoencoder_model = autoencoder
            self.encoder_model = encoder
            
            return autoencoder, encoder, None

        except Exception as e:
            return None, None, f"ì˜¤í† ì¸ì½”ë” ìƒì„± ì‹¤íŒ¨: {str(e)}"

    def train_autoencoder(self, autoencoder, data, epochs=None, progress_bar=None, status_text=None):
        """ì˜¤í† ì¸ì½”ë” í›ˆë ¨"""
        epochs = epochs or DeepLearningConfig.AUTOENCODER_EPOCHS
        
        try:
            callbacks = []
            
            # Streamlit ì½œë°± (í•„ìš”ì‹œ)
            if progress_bar and status_text:
                class AutoencoderProgressCallback(keras.callbacks.Callback):
                    def __init__(self, total_epochs, progress_bar, status_text):
                        super().__init__()
                        self.total_epochs = total_epochs
                        self.progress_bar = progress_bar
                        self.status_text = status_text

                    def on_epoch_end(self, epoch, logs=None):
                        progress = (epoch + 1) / self.total_epochs
                        self.progress_bar.progress(progress)
                        
                        if logs:
                            self.status_text.text(
                                f"ì—í¬í¬ {epoch + 1}/{self.total_epochs} - ì†ì‹¤: {logs.get('loss', 0):.4f}"
                            )

                progress_callback = AutoencoderProgressCallback(epochs, progress_bar, status_text)
                callbacks.append(progress_callback)

            # í›ˆë ¨ ì‹¤í–‰
            history = autoencoder.fit(
                data, data,
                epochs=epochs,
                batch_size=DeepLearningConfig.BATCH_SIZE,
                validation_split=DeepLearningConfig.VALIDATION_SPLIT,
                verbose=0,
                callbacks=callbacks
            )

            return history, None

        except Exception as e:
            return None, f"ì˜¤í† ì¸ì½”ë” í›ˆë ¨ ì‹¤íŒ¨: {str(e)}"


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ê¸°ì¡´ í•¨ìˆ˜ì™€ì˜ í˜¸í™˜ì„±)
dl_models = DeepLearningModels()

# ê¸°ì¡´ í•¨ìˆ˜ë“¤ê³¼ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼ë“¤
def create_safe_classification_model(input_dim, n_clusters, hidden_units, dropout_rate, learning_rate):
    return dl_models.create_safe_classification_model(input_dim, n_clusters, hidden_units, dropout_rate, learning_rate)

def train_model_with_progress(model, X_train, y_train, X_test, y_test, epochs, progress_bar, status_text):
    return dl_models.train_model_with_progress(model, X_train, y_train, X_test, y_test, epochs, progress_bar, status_text)

def display_model_architecture_info(hidden_units, dropout_rate, n_clusters):
    return dl_models.display_model_architecture_info(hidden_units, dropout_rate, n_clusters)

def evaluate_and_display_results(model, X_test, y_test, history, n_clusters):
    return dl_models.evaluate_and_display_results(model, X_test, y_test, history, n_clusters)
