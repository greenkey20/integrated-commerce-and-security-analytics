"""
ë”¥ëŸ¬ë‹ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ëª¨ë“ˆ

"í˜¼ì ê³µë¶€í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹, ë”¥ëŸ¬ë‹" ë°©ì‹ì„ ë”¥ëŸ¬ë‹ì— ì ìš©í•œ ì‹¤ìŠµ ì½”ë“œ
í•™ìŠµ ëª©ì ìœ¼ë¡œ ë‹¨ê³„ë³„ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ ìˆ˜í–‰
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from itertools import product
import time
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# í”„ë¡œì íŠ¸ ë‚´ë¶€ ëª¨ë“ˆ import
from core.segmentation.data_processing import CustomerDataProcessor
from core.segmentation.models import DeepLearningModels
from config.settings import DeepLearningConfig
import streamlit as st


class HyperparameterTuner:
    """í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.data_processor = CustomerDataProcessor()
        self.dl_models = DeepLearningModels()
        self.results_history = []
        
    def prepare_data(self):
        """ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
        print("ğŸ“Š ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ì¤‘...")
        
        # ë°ì´í„° ë¡œë“œ
        data = self.data_processor.load_data()
        print(f"ë°ì´í„° í¬ê¸°: {data.shape}")
        
        # íŠ¹ì„± ì¶”ì¶œ ë° ìŠ¤ì¼€ì¼ë§
        features = data[["Age", "Annual Income (k$)", "Spending Score (1-100)"]]
        scaler = StandardScaler()
        X = scaler.fit_transform(features)
        
        # ì„ì‹œ ë ˆì´ë¸” ìƒì„± (K-meansë¡œ í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰)
        from sklearn.cluster import KMeans
        kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)
        y = kmeans.fit_predict(X)
        
        # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y, shuffle=True
        )
        
        print(f"í›ˆë ¨ ì„¸íŠ¸: {X_train.shape}, í…ŒìŠ¤íŠ¸ ì„¸íŠ¸: {X_test.shape}")
        print(f"í´ëŸ¬ìŠ¤í„° ë¶„í¬: {np.bincount(y)}")
        
        return X_train, X_test, y_train, y_test
    
    def tune_learning_rate(self, X_train, y_train, X_test, y_test):
        """í•™ìŠµë¥  íŠœë‹ - ê°€ì¥ ì¤‘ìš”í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°"""
        
        learning_rates = [0.1, 0.01, 0.001, 0.0001]  # 10ë°°ì”© ì¤„ì—¬ê°€ë©° í…ŒìŠ¤íŠ¸
        results = []
        
        print("\\n=== 1ë‹¨ê³„: Learning Rate íŠœë‹ ===")
        print("í•™ìŠµë¥ ì€ ëª¨ë¸ ì„±ëŠ¥ì— ê°€ì¥ í° ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„°ì•¼!")
        
        for lr in learning_rates:
            print(f"\\nğŸ”§ í•™ìŠµë¥  {lr} í…ŒìŠ¤íŠ¸ ì¤‘...")
            
            try:
                # ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ëª¨ë¸ ìƒì„± (ë‹¤ë¥¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°ëŠ” ê³ ì •)
                model, error = self.dl_models.create_safe_classification_model(
                    input_dim=3, 
                    n_clusters=5,
                    hidden_units=64,     # ê³ ì •ê°’
                    dropout_rate=0.2,    # ê³ ì •ê°’  
                    learning_rate=lr     # í…ŒìŠ¤íŠ¸í•  ê°’
                )
                
                if error:
                    print(f"âŒ ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {error}")
                    continue
                    
                # ì§§ì€ ì—í¬í¬ë¡œ ë¹ ë¥´ê²Œ í…ŒìŠ¤íŠ¸ (30 ì—í¬í¬)
                history, error = self.dl_models.train_model_with_progress(
                    model, X_train, y_train, X_test, y_test, 
                    epochs=30, progress_bar=None, status_text=None
                )
                
                if error:
                    print(f"âŒ í›ˆë ¨ ì‹¤íŒ¨: {error}")
                    continue
                    
                # ìµœì¢… ê²€ì¦ ì •í™•ë„ ê¸°ë¡
                final_val_accuracy = max(history.history['val_accuracy'])
                
                results.append({
                    'learning_rate': lr,
                    'val_accuracy': final_val_accuracy,
                    'history': history.history
                })
                
                print(f"âœ… í•™ìŠµë¥  {lr}: ìµœê³  ê²€ì¦ ì •í™•ë„ = {final_val_accuracy:.4f}")
                
                # í•™ìŠµ ê³¼ì • ê°„ë‹¨ ë¶„ì„
                if lr >= 0.1:
                    print("   â†’ í•™ìŠµë¥ ì´ ë„ˆë¬´ ë†’ì„ ìˆ˜ ìˆìŒ (ë°œì‚° ìœ„í—˜)")
                elif lr <= 0.0001:
                    print("   â†’ í•™ìŠµë¥ ì´ ë„ˆë¬´ ë‚®ì„ ìˆ˜ ìˆìŒ (í•™ìŠµ ì†ë„ ëŠë¦¼)")
                else:
                    print("   â†’ ì ì ˆí•œ í•™ìŠµë¥  ë²”ìœ„")
                    
            except Exception as e:
                print(f"âŒ ì˜ˆì™¸ ë°œìƒ: {str(e)}")
                continue
        
        if not results:
            print("âŒ ëª¨ë“  í•™ìŠµë¥  í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
            return None, []
            
        # ìµœì  í•™ìŠµë¥  ì„ íƒ
        best_lr_result = max(results, key=lambda x: x['val_accuracy'])
        best_lr = best_lr_result['learning_rate']
        
        print(f"\\nğŸ¯ ìµœì  í•™ìŠµë¥ : {best_lr} (ì •í™•ë„: {best_lr_result['val_accuracy']:.4f})")
        
        return best_lr, results

    def tune_hidden_units(self, X_train, y_train, X_test, y_test, best_lr):
        """ì€ë‹‰ì¸µ ë‰´ëŸ° ìˆ˜ íŠœë‹"""
        
        hidden_units_list = [16, 32, 64, 128, 256]  # 2ë°°ì”© ëŠ˜ë ¤ê°€ë©° í…ŒìŠ¤íŠ¸
        results = []
        
        print(f"\\n=== 2ë‹¨ê³„: Hidden Units íŠœë‹ (ìµœì  í•™ìŠµë¥  {best_lr} ì‚¬ìš©) ===")
        print("ì€ë‹‰ì¸µ í¬ê¸°ëŠ” ëª¨ë¸ì˜ í‘œí˜„ë ¥ì„ ê²°ì •í•´!")
        
        for units in hidden_units_list:
            print(f"\\nğŸ”§ ì€ë‹‰ì¸µ {units}ê°œ ë‰´ëŸ° í…ŒìŠ¤íŠ¸ ì¤‘...")
            
            try:
                model, error = self.dl_models.create_safe_classification_model(
                    input_dim=3,
                    n_clusters=5, 
                    hidden_units=units,     # í…ŒìŠ¤íŠ¸í•  ê°’
                    dropout_rate=0.2,       # ê³ ì •ê°’
                    learning_rate=best_lr   # 1ë‹¨ê³„ì—ì„œ ì°¾ì€ ìµœì ê°’
                )
                
                if error:
                    print(f"âŒ ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {error}")
                    continue
                    
                history, error = self.dl_models.train_model_with_progress(
                    model, X_train, y_train, X_test, y_test,
                    epochs=50, progress_bar=None, status_text=None
                )
                
                if error:
                    print(f"âŒ í›ˆë ¨ ì‹¤íŒ¨: {error}")
                    continue
                    
                final_val_accuracy = max(history.history['val_accuracy'])
                results.append({
                    'hidden_units': units,
                    'val_accuracy': final_val_accuracy,
                    'history': history.history
                })
                
                print(f"âœ… ì€ë‹‰ì¸µ {units}ê°œ: ìµœê³  ê²€ì¦ ì •í™•ë„ = {final_val_accuracy:.4f}")
                
                # ëª¨ë¸ ë³µì¡ë„ ë¶„ì„
                if units <= 32:
                    print("   â†’ ì‘ì€ ëª¨ë¸: ë¹ ë¥´ì§€ë§Œ í‘œí˜„ë ¥ ì œí•œì ")
                elif units >= 256:
                    print("   â†’ í° ëª¨ë¸: í‘œí˜„ë ¥ ë†’ì§€ë§Œ ê³¼ì í•© ìœ„í—˜")
                else:
                    print("   â†’ ì ì ˆí•œ í¬ê¸°: ê· í˜• ì¡íŒ ëª¨ë¸")
                    
            except Exception as e:
                print(f"âŒ ì˜ˆì™¸ ë°œìƒ: {str(e)}")
                continue
        
        if not results:
            print("âŒ ëª¨ë“  ì€ë‹‰ì¸µ í¬ê¸° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
            return None, []
            
        best_units_result = max(results, key=lambda x: x['val_accuracy'])
        best_units = best_units_result['hidden_units']
        
        print(f"\\nğŸ¯ ìµœì  ì€ë‹‰ì¸µ í¬ê¸°: {best_units} (ì •í™•ë„: {best_units_result['val_accuracy']:.4f})")
        
        return best_units, results

    def tune_dropout_rate(self, X_train, y_train, X_test, y_test, best_lr, best_units):
        """ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨ íŠœë‹"""
        
        dropout_rates = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]  # 0%ë¶€í„° 50%ê¹Œì§€
        results = []
        
        print(f"\\n=== 3ë‹¨ê³„: Dropout Rate íŠœë‹ (í•™ìŠµë¥  {best_lr}, ì€ë‹‰ì¸µ {best_units} ì‚¬ìš©) ===")
        print("ë“œë¡­ì•„ì›ƒì€ ê³¼ì í•©ì„ ë°©ì§€í•˜ëŠ” í•µì‹¬ ê¸°ë²•ì´ì•¼!")
        
        for dropout in dropout_rates:
            print(f"\\nğŸ”§ ë“œë¡­ì•„ì›ƒ {dropout:.1f} í…ŒìŠ¤íŠ¸ ì¤‘...")
            
            try:
                model, error = self.dl_models.create_safe_classification_model(
                    input_dim=3,
                    n_clusters=5,
                    hidden_units=best_units,  # 2ë‹¨ê³„ì—ì„œ ì°¾ì€ ìµœì ê°’
                    dropout_rate=dropout,     # í…ŒìŠ¤íŠ¸í•  ê°’
                    learning_rate=best_lr     # 1ë‹¨ê³„ì—ì„œ ì°¾ì€ ìµœì ê°’
                )
                
                if error:
                    print(f"âŒ ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {error}")
                    continue
                    
                history, error = self.dl_models.train_model_with_progress(
                    model, X_train, y_train, X_test, y_test,
                    epochs=100, progress_bar=None, status_text=None
                )
                
                if error:
                    print(f"âŒ í›ˆë ¨ ì‹¤íŒ¨: {error}")
                    continue
                    
                final_val_accuracy = max(history.history['val_accuracy'])
                final_train_accuracy = max(history.history['accuracy'])
                
                results.append({
                    'dropout_rate': dropout,
                    'val_accuracy': final_val_accuracy,
                    'train_accuracy': final_train_accuracy,
                    'overfitting_gap': final_train_accuracy - final_val_accuracy,
                    'history': history.history
                })
                
                print(f"âœ… ë“œë¡­ì•„ì›ƒ {dropout:.1f}: ê²€ì¦ ì •í™•ë„ = {final_val_accuracy:.4f}")
                print(f"   í›ˆë ¨ ì •í™•ë„ = {final_train_accuracy:.4f}, ê³¼ì í•© ê°­ = {final_train_accuracy - final_val_accuracy:.4f}")
                
                # ê³¼ì í•© ë¶„ì„
                if dropout == 0.0:
                    print("   â†’ ë“œë¡­ì•„ì›ƒ ì—†ìŒ: ê³¼ì í•© ìœ„í—˜ ìˆìŒ")
                elif dropout >= 0.4:
                    print("   â†’ ë†’ì€ ë“œë¡­ì•„ì›ƒ: ê³¼ì í•© ë°©ì§€ ê°•í™”, í‘œí˜„ë ¥ ê°ì†Œ")
                else:
                    print("   â†’ ì ì ˆí•œ ë“œë¡­ì•„ì›ƒ: ê· í˜• ì¡íŒ ì •ê·œí™”")
                    
            except Exception as e:
                print(f"âŒ ì˜ˆì™¸ ë°œìƒ: {str(e)}")
                continue
        
        if not results:
            print("âŒ ëª¨ë“  ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
            return None, []
            
        best_dropout_result = max(results, key=lambda x: x['val_accuracy'])
        best_dropout = best_dropout_result['dropout_rate']
        
        print(f"\\nğŸ¯ ìµœì  ë“œë¡­ì•„ì›ƒ: {best_dropout} (ì •í™•ë„: {best_dropout_result['val_accuracy']:.4f})")
        
        return best_dropout, results

    def final_validation(self, X_train, y_train, X_test, y_test, 
                        best_lr, best_units, best_dropout):
        """ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… ëª¨ë¸ í›ˆë ¨ ë° ê²€ì¦"""
        
        print(f"\\n=== ğŸ† ìµœì¢… ê²€ì¦ ===")
        print(f"ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°:")
        print(f"- Learning Rate: {best_lr}")
        print(f"- Hidden Units: {best_units}")  
        print(f"- Dropout Rate: {best_dropout}")
        
        try:
            # ìµœì  ì„¤ì •ìœ¼ë¡œ ëª¨ë¸ ìƒì„±
            final_model, error = self.dl_models.create_safe_classification_model(
                input_dim=3,
                n_clusters=5,
                hidden_units=best_units,
                dropout_rate=best_dropout,
                learning_rate=best_lr
            )
            
            if error:
                print(f"âŒ ìµœì¢… ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {error}")
                return None, None
                
            print("\\nğŸš€ ìµœì¢… ëª¨ë¸ í›ˆë ¨ ì‹œì‘ (ì¶©ë¶„í•œ ì—í¬í¬ë¡œ)...")
            
            # ì¶©ë¶„í•œ ì—í¬í¬ë¡œ ìµœì¢… í›ˆë ¨
            final_history, error = self.dl_models.train_model_with_progress(
                final_model, X_train, y_train, X_test, y_test,
                epochs=200, progress_bar=None, status_text=None
            )
            
            if error:
                print(f"âŒ ìµœì¢… í›ˆë ¨ ì‹¤íŒ¨: {error}")
                return None, None
                
            # ìµœì¢… ì„±ëŠ¥ í‰ê°€
            test_loss, test_accuracy = final_model.evaluate(X_test, y_test, verbose=0)
            
            print(f"\\nğŸ† ìµœì¢… í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_accuracy:.4f}")
            print(f"ğŸ† ìµœì¢… í…ŒìŠ¤íŠ¸ ì†ì‹¤: {test_loss:.4f}")
            
            # í›ˆë ¨ ê³¼ì • ìš”ì•½
            final_train_acc = max(final_history.history['accuracy'])
            final_val_acc = max(final_history.history['val_accuracy'])
            print(f"ğŸ† ìµœê³  í›ˆë ¨ ì •í™•ë„: {final_train_acc:.4f}")
            print(f"ğŸ† ìµœê³  ê²€ì¦ ì •í™•ë„: {final_val_acc:.4f}")
            print(f"ğŸ† ê³¼ì í•© ê°­: {final_train_acc - final_val_acc:.4f}")
            
            return final_model, final_history
            
        except Exception as e:
            print(f"âŒ ìµœì¢… ê²€ì¦ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}")
            return None, None

    def save_results_to_csv(self, results_summary, filename=None):
        """ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥"""
        if filename is None:
            timestamp = int(time.time())
            filename = f"hyperparameter_tuning_results_{timestamp}.csv"
            
        try:
            df = pd.DataFrame(results_summary)
            df.to_csv(filename, index=False, encoding='utf-8')
            print(f"\\nğŸ’¾ ê²°ê³¼ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {str(e)}")

    def run_complete_tuning(self, save_results=True):
        """ì „ì²´ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        
        print("ğŸŒ± ë”¥ëŸ¬ë‹ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘!")
        print("=" * 60)
        print("ì´ ê³¼ì •ì€ 'í˜¼ì ê³µë¶€í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹, ë”¥ëŸ¬ë‹' ë°©ì‹ì„ ë”¥ëŸ¬ë‹ì— ì ìš©í•œ ê²ƒì…ë‹ˆë‹¤.")
        print("=" * 60)
        
        start_time = time.time()
        
        try:
            # 1. ë°ì´í„° ì¤€ë¹„
            X_train, X_test, y_train, y_test = self.prepare_data()
            
            # 2. í•™ìŠµë¥  íŠœë‹
            best_lr, lr_results = self.tune_learning_rate(X_train, y_train, X_test, y_test)
            if best_lr is None:
                print("âŒ í•™ìŠµë¥  íŠœë‹ ì‹¤íŒ¨, ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                return None
            
            # 3. ì€ë‹‰ì¸µ í¬ê¸° íŠœë‹  
            best_units, units_results = self.tune_hidden_units(X_train, y_train, X_test, y_test, best_lr)
            if best_units is None:
                print("âŒ ì€ë‹‰ì¸µ íŠœë‹ ì‹¤íŒ¨, ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                return None
            
            # 4. ë“œë¡­ì•„ì›ƒ íŠœë‹
            best_dropout, dropout_results = self.tune_dropout_rate(X_train, y_train, X_test, y_test, best_lr, best_units)
            if best_dropout is None:
                print("âŒ ë“œë¡­ì•„ì›ƒ íŠœë‹ ì‹¤íŒ¨, ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                return None
            
            # 5. ìµœì¢… ê²€ì¦
            final_model, final_history = self.final_validation(
                X_train, y_train, X_test, y_test, best_lr, best_units, best_dropout
            )
            
            end_time = time.time()
            print(f"\\nâ° ì´ íŠœë‹ ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")
            
            # ê²°ê³¼ ìš”ì•½
            results_summary = {
                'best_learning_rate': best_lr,
                'best_hidden_units': best_units, 
                'best_dropout_rate': best_dropout,
                'final_test_accuracy': final_model.evaluate(X_test, y_test, verbose=0)[1] if final_model else None,
                'tuning_time_seconds': end_time - start_time
            }
            
            # ê²°ê³¼ ì €ì¥
            if save_results and final_model:
                self.save_results_to_csv([results_summary])
            
            print("\\nğŸ‰ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì™„ë£Œ!")
            
            return {
                'best_learning_rate': best_lr,
                'best_hidden_units': best_units, 
                'best_dropout_rate': best_dropout,
                'final_model': final_model,
                'final_history': final_history,
                'lr_results': lr_results,
                'units_results': units_results,
                'dropout_results': dropout_results
            }
            
        except Exception as e:
            print(f"âŒ íŠœë‹ í”„ë¡œì„¸ìŠ¤ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}")
            return None


def run_grid_search_experiment(limited_combinations=True):
    """ê·¸ë¦¬ë“œ ì„œì¹˜ ì‹¤í—˜ ì‹¤í–‰ (ì‚¬ì´í‚·ëŸ° GridSearchCV ë°©ì‹ê³¼ ìœ ì‚¬)"""
    
    print("\\nğŸ” ê·¸ë¦¬ë“œ ì„œì¹˜ ì‹¤í—˜ ì‹œì‘!")
    print("=" * 40)
    
    tuner = HyperparameterTuner()
    X_train, X_test, y_train, y_test = tuner.prepare_data()
    
    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì •ì˜
    if limited_combinations:
        # ì œí•œëœ ì¡°í•© (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©)
        param_grid = {
            'learning_rate': [0.01, 0.001],
            'hidden_units': [32, 64],
            'dropout_rate': [0.1, 0.2]
        }
    else:
        # ì „ì²´ ì¡°í•© (ìƒì„¸ ë¶„ì„ìš©)
        param_grid = {
            'learning_rate': [0.01, 0.001, 0.0001],
            'hidden_units': [32, 64, 128],
            'dropout_rate': [0.1, 0.2, 0.3]
        }
    
    # ëª¨ë“  ì¡°í•© ìƒì„± (itertools.product ì‚¬ìš©)
    combinations = list(product(
        param_grid['learning_rate'],
        param_grid['hidden_units'], 
        param_grid['dropout_rate']
    ))
    
    print(f"ì´ {len(combinations)}ê°œ ì¡°í•© í…ŒìŠ¤íŠ¸ ì˜ˆì •...")
    
    results = []
    
    for i, (lr, units, dropout) in enumerate(combinations, 1):
        print(f"\\n[{i}/{len(combinations)}] lr={lr}, units={units}, dropout={dropout}")
        
        try:
            model, error = tuner.dl_models.create_safe_classification_model(
                input_dim=3, n_clusters=5,
                hidden_units=units, dropout_rate=dropout, learning_rate=lr
            )
            
            if error:
                print(f"âŒ ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {error}")
                continue
                
            history, error = tuner.dl_models.train_model_with_progress(
                model, X_train, y_train, X_test, y_test,
                epochs=50, progress_bar=None, status_text=None
            )
            
            if error:
                print(f"âŒ í›ˆë ¨ ì‹¤íŒ¨: {error}")
                continue
                
            val_accuracy = max(history.history['val_accuracy'])
            train_accuracy = max(history.history['accuracy'])
            
            results.append({
                'learning_rate': lr,
                'hidden_units': units,
                'dropout_rate': dropout,
                'val_accuracy': val_accuracy,
                'train_accuracy': train_accuracy,
                'overfitting_gap': train_accuracy - val_accuracy
            })
            
            print(f"âœ… ê²€ì¦ ì •í™•ë„: {val_accuracy:.4f}, ê³¼ì í•© ê°­: {train_accuracy - val_accuracy:.4f}")
            
        except Exception as e:
            print(f"âŒ ì˜ˆì™¸ ë°œìƒ: {str(e)}")
            continue
    
    if not results:
        print("âŒ ëª¨ë“  ì¡°í•© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
        return None
        
    # ìµœì  ì¡°í•© ì°¾ê¸°
    best_result = max(results, key=lambda x: x['val_accuracy'])
    print(f"\\nğŸ† ê·¸ë¦¬ë“œ ì„œì¹˜ ìµœì  ì¡°í•©:")
    print(f"Learning Rate: {best_result['learning_rate']}")
    print(f"Hidden Units: {best_result['hidden_units']}")  
    print(f"Dropout Rate: {best_result['dropout_rate']}")
    print(f"ê²€ì¦ ì •í™•ë„: {best_result['val_accuracy']:.4f}")
    print(f"ê³¼ì í•© ê°­: {best_result['overfitting_gap']:.4f}")
    
    # ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ì •ë¦¬
    results_df = pd.DataFrame(results)
    print(f"\\nğŸ“Š ìƒìœ„ 5ê°œ ì¡°í•©:")
    print(results_df.nlargest(5, 'val_accuracy')[['learning_rate', 'hidden_units', 'dropout_rate', 'val_accuracy', 'overfitting_gap']])
    
    return best_result, results_df


# ì‹¤í–‰ í•¨ìˆ˜ë“¤
def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ¯ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹¤í—˜ì„ ì„ íƒí•˜ì„¸ìš”:")
    print("1. ë‹¨ê³„ë³„ íŠœë‹ (ê¶Œì¥)")
    print("2. ê·¸ë¦¬ë“œ ì„œì¹˜ (ì œí•œëœ ì¡°í•©)")  
    print("3. ê·¸ë¦¬ë“œ ì„œì¹˜ (ì „ì²´ ì¡°í•©)")
    
    choice = input("\\nì„ íƒ (1/2/3): ").strip()
    
    if choice == "1":
        tuner = HyperparameterTuner()
        results = tuner.run_complete_tuning()
        
    elif choice == "2":
        results = run_grid_search_experiment(limited_combinations=True)
        
    elif choice == "3":
        results = run_grid_search_experiment(limited_combinations=False)
        
    else:
        print("ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")
        return
    
    if results:
        print("\\nâœ… ì‹¤í—˜ ì™„ë£Œ!")
    else:
        print("\\nâŒ ì‹¤í—˜ ì‹¤íŒ¨!")


if __name__ == "__main__":
    # TensorFlow ê²½ê³  ë©”ì‹œì§€ ì¤„ì´ê¸°
    import os
    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
    
    main()
