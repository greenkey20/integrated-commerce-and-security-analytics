# core/security_hyperparameter_tuning.py
"""
í•˜ì´ë¸Œë¦¬ë“œ ë³´ì•ˆ ì‹œìŠ¤í…œìš© í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
MLP + CNN + Ensemble ëª¨ë¸ì˜ ìµœì  íŒŒë¼ë¯¸í„° íƒìƒ‰
"""

import numpy as np
import pandas as pd
import time
from itertools import product
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import json
from datetime import datetime

from core.anomaly_detection import APILogAnomalyDetector
from data.cicids_data_loader import CICIDSDataLoader


class SecurityHyperparameterTuner:
    """ë³´ì•ˆ ì‹œìŠ¤í…œìš© í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë„ˆ"""
    
    def __init__(self):
        self.data_loader = CICIDSDataLoader()
        self.results_history = []
        
    def prepare_security_data(self):
        """ë³´ì•ˆ ë°ì´í„° ì¤€ë¹„"""
        print("ğŸ” ë³´ì•ˆ ë°ì´í„° ì¤€ë¹„ ì¤‘...")
        
        # CICIDS2017 ìŠ¤íƒ€ì¼ ë°ì´í„° ìƒì„±
        df = self.data_loader.download_sample_data()
        processed_df = self.data_loader.preprocess_for_api_logs(df)
        
        # íŠ¹ì„±ê³¼ ë¼ë²¨ ë¶„ë¦¬
        feature_columns = [col for col in processed_df.columns if col not in ['Label', 'Original_Label']]
        X = processed_df[feature_columns].values
        y = processed_df['Label'].values
        
        # í›ˆë ¨/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë¶„í•  (6:2:2)
        X_temp, X_test, y_temp, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        X_train, X_val, y_train, y_val = train_test_split(
            X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp
        )
        
        print(f"í›ˆë ¨ ì„¸íŠ¸: {X_train.shape[0]} ìƒ˜í”Œ")
        print(f"ê²€ì¦ ì„¸íŠ¸: {X_val.shape[0]} ìƒ˜í”Œ") 
        print(f"í…ŒìŠ¤íŠ¸ ì„¸íŠ¸: {X_test.shape[0]} ìƒ˜í”Œ")
        print(f"ì •ìƒ/ê³µê²© ë¹„ìœ¨: {np.bincount(y_train)}")
        
        return X_train, X_val, X_test, y_train, y_val, y_test
    
    def tune_mlp_hyperparameters(self, X_train, y_train, X_val, y_val):
        """MLP ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹"""
        
        print("\nğŸ§  MLP í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘...")
        
        # MLP í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ
        mlp_param_grid = {
            'hidden_units': [64, 128, 256],
            'dropout_rate': [0.2, 0.3, 0.4],
            'learning_rate': [0.001, 0.0001],
            'epochs': [50, 100]
        }
        
        mlp_results = []
        combinations = list(product(*mlp_param_grid.values()))
        
        print(f"ì´ {len(combinations)}ê°œ MLP ì¡°í•© í…ŒìŠ¤íŠ¸...")
        
        for i, (units, dropout, lr, epochs) in enumerate(combinations, 1):
            print(f"\n[{i}/{len(combinations)}] MLP: units={units}, dropout={dropout}, lr={lr}, epochs={epochs}")
            
            try:
                # MLP ëª¨ë¸ ìƒì„± ë° í›ˆë ¨
                detector = APILogAnomalyDetector(model_type='mlp')
                
                # ì„ì‹œë¡œ ìŠ¤ì¼€ì¼ëŸ¬ ì ìš©
                detector.scaler.fit(X_train)
                X_train_scaled = detector.scaler.transform(X_train)
                X_val_scaled = detector.scaler.transform(X_val)
                
                # ëª¨ë¸ ìƒì„±
                model = detector.build_mlp_model(X_train.shape[1])
                
                # í›ˆë ¨
                history = model.fit(
                    X_train_scaled, y_train,
                    validation_data=(X_val_scaled, y_val),
                    epochs=epochs,
                    batch_size=32,
                    verbose=0
                )
                
                # ì„±ëŠ¥ í‰ê°€
                val_loss, val_accuracy = model.evaluate(X_val_scaled, y_val, verbose=0)
                
                # ì˜ˆì¸¡ ë° ìƒì„¸ ë©”íŠ¸ë¦­
                y_pred = (model.predict(X_val_scaled) > 0.5).astype(int).flatten()
                
                mlp_results.append({
                    'model_type': 'MLP',
                    'hidden_units': units,
                    'dropout_rate': dropout,
                    'learning_rate': lr,
                    'epochs': epochs,
                    'val_accuracy': val_accuracy,
                    'val_loss': val_loss,
                    'final_train_acc': history.history['accuracy'][-1],
                    'overfitting_gap': history.history['accuracy'][-1] - val_accuracy
                })
                
                print(f"âœ… ê²€ì¦ ì •í™•ë„: {val_accuracy:.4f}, ì†ì‹¤: {val_loss:.4f}")
                
            except Exception as e:
                print(f"âŒ MLP ì‹¤í—˜ ì‹¤íŒ¨: {str(e)}")
                continue
        
        if mlp_results:
            best_mlp = max(mlp_results, key=lambda x: x['val_accuracy'])
            print(f"\nğŸ† ìµœì  MLP ì„¤ì •:")
            print(f"  Hidden Units: {best_mlp['hidden_units']}")
            print(f"  Dropout: {best_mlp['dropout_rate']}")
            print(f"  Learning Rate: {best_mlp['learning_rate']}")
            print(f"  Epochs: {best_mlp['epochs']}")
            print(f"  ê²€ì¦ ì •í™•ë„: {best_mlp['val_accuracy']:.4f}")
            
            return best_mlp, mlp_results
        else:
            print("âŒ MLP íŠœë‹ ì‹¤íŒ¨")
            return None, []
    
    def tune_cnn_hyperparameters(self, X_train, y_train, X_val, y_val):
        """CNN ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹"""
        
        print("\nğŸ“Š CNN í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘...")
        
        # CNN í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ (ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ìš©)
        cnn_param_grid = {
            'conv_filters': [32, 64],
            'conv_kernel': [3, 5],
            'sequence_length': [5, 10, 15],
            'learning_rate': [0.001, 0.0001],
            'epochs': [50, 100]
        }
        
        cnn_results = []
        combinations = list(product(*cnn_param_grid.values()))
        
        print(f"ì´ {len(combinations)}ê°œ CNN ì¡°í•© í…ŒìŠ¤íŠ¸...")
        
        for i, (filters, kernel, seq_len, lr, epochs) in enumerate(combinations, 1):
            print(f"\n[{i}/{len(combinations)}] CNN: filters={filters}, kernel={kernel}, seq_len={seq_len}")
            
            try:
                # CNN ëª¨ë¸ ìƒì„± ë° í›ˆë ¨
                detector = APILogAnomalyDetector(model_type='cnn')
                detector.sequence_length = seq_len
                
                # ì‹œí€€ìŠ¤ ë°ì´í„° ì¤€ë¹„
                detector.scaler.fit(X_train)
                X_train_scaled = detector.scaler.transform(X_train)
                X_val_scaled = detector.scaler.transform(X_val)
                
                # CNNìš© ì‹œí€€ìŠ¤ ë³€í™˜
                X_train_seq = detector.prepare_sequence_data(X_train_scaled)
                X_val_seq = detector.prepare_sequence_data(X_val_scaled)
                y_train_seq = y_train[seq_len-1:]
                y_val_seq = y_val[seq_len-1:]
                
                if len(X_train_seq) < 50:  # ì‹œí€€ìŠ¤ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ìŠ¤í‚µ
                    print(f"âŒ ì‹œí€€ìŠ¤ ê¸¸ì´ ë¶€ì¡±: {len(X_train_seq)}")
                    continue
                
                # CNN ëª¨ë¸ ìƒì„±
                model = detector.build_cnn_model(X_train.shape[1])
                
                # í›ˆë ¨
                history = model.fit(
                    X_train_seq, y_train_seq,
                    validation_data=(X_val_seq, y_val_seq),
                    epochs=epochs,
                    batch_size=32,
                    verbose=0
                )
                
                # ì„±ëŠ¥ í‰ê°€
                val_loss, val_accuracy = model.evaluate(X_val_seq, y_val_seq, verbose=0)
                
                cnn_results.append({
                    'model_type': 'CNN',
                    'conv_filters': filters,
                    'conv_kernel': kernel,
                    'sequence_length': seq_len,
                    'learning_rate': lr,
                    'epochs': epochs,
                    'val_accuracy': val_accuracy,
                    'val_loss': val_loss,
                    'final_train_acc': history.history['accuracy'][-1],
                    'overfitting_gap': history.history['accuracy'][-1] - val_accuracy
                })
                
                print(f"âœ… ê²€ì¦ ì •í™•ë„: {val_accuracy:.4f}, ì†ì‹¤: {val_loss:.4f}")
                
            except Exception as e:
                print(f"âŒ CNN ì‹¤í—˜ ì‹¤íŒ¨: {str(e)}")
                continue
        
        if cnn_results:
            best_cnn = max(cnn_results, key=lambda x: x['val_accuracy'])
            print(f"\nğŸ† ìµœì  CNN ì„¤ì •:")
            print(f"  Conv Filters: {best_cnn['conv_filters']}")
            print(f"  Kernel Size: {best_cnn['conv_kernel']}")
            print(f"  Sequence Length: {best_cnn['sequence_length']}")
            print(f"  Learning Rate: {best_cnn['learning_rate']}")
            print(f"  Epochs: {best_cnn['epochs']}")
            print(f"  ê²€ì¦ ì •í™•ë„: {best_cnn['val_accuracy']:.4f}")
            
            return best_cnn, cnn_results
        else:
            print("âŒ CNN íŠœë‹ ì‹¤íŒ¨")
            return None, []
    
    def tune_hybrid_model(self, X_train, y_train, X_val, y_val, best_mlp, best_cnn):
        """í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ íŒŒë¼ë¯¸í„° íŠœë‹"""
        
        print("\nğŸ”¥ í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ íŠœë‹ ì‹œì‘...")
        
        if not best_mlp or not best_cnn:
            print("âŒ MLP ë˜ëŠ” CNN ìµœì  íŒŒë¼ë¯¸í„°ê°€ ì—†ì–´ í•˜ì´ë¸Œë¦¬ë“œ íŠœë‹ ë¶ˆê°€")
            return None, []
        
        # í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ íŒŒë¼ë¯¸í„° (best MLP + CNN ì¡°í•©)
        hybrid_params = [
            {
                'mlp_units': best_mlp['hidden_units'],
                'mlp_dropout': best_mlp['dropout_rate'],
                'cnn_filters': best_cnn['conv_filters'],
                'sequence_length': best_cnn['sequence_length'],
                'fusion_units': 32,
                'learning_rate': 0.001,
                'epochs': 150
            },
            {
                'mlp_units': best_mlp['hidden_units'],
                'mlp_dropout': best_mlp['dropout_rate'], 
                'cnn_filters': best_cnn['conv_filters'],
                'sequence_length': best_cnn['sequence_length'],
                'fusion_units': 64,
                'learning_rate': 0.0001,
                'epochs': 200
            }
        ]
        
        hybrid_results = []
        
        for i, params in enumerate(hybrid_params, 1):
            print(f"\n[{i}/{len(hybrid_params)}] í•˜ì´ë¸Œë¦¬ë“œ ì¡°í•© {i} í…ŒìŠ¤íŠ¸...")
            
            try:
                # í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ìƒì„±
                detector = APILogAnomalyDetector(model_type='hybrid')
                detector.sequence_length = params['sequence_length']
                
                # ë°ì´í„° ì¤€ë¹„
                detector.scaler.fit(X_train)
                X_train_scaled = detector.scaler.transform(X_train)
                X_val_scaled = detector.scaler.transform(X_val)
                
                # MLPìš© ê°œë³„ ë°ì´í„°ì™€ CNNìš© ì‹œí€€ìŠ¤ ë°ì´í„°
                X_train_seq = detector.prepare_sequence_data(X_train_scaled)
                X_val_seq = detector.prepare_sequence_data(X_val_scaled)
                X_train_ind = X_train_scaled[params['sequence_length']-1:]
                X_val_ind = X_val_scaled[params['sequence_length']-1:]
                y_train_hybrid = y_train[params['sequence_length']-1:]
                y_val_hybrid = y_val[params['sequence_length']-1:]
                
                # í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ êµ¬ì¶•
                model = detector.build_hybrid_model(X_train.shape[1])
                
                # í›ˆë ¨
                history = model.fit(
                    [X_train_ind, X_train_seq], y_train_hybrid,
                    validation_data=([X_val_ind, X_val_seq], y_val_hybrid),
                    epochs=params['epochs'],
                    batch_size=32,
                    verbose=0
                )
                
                # ì„±ëŠ¥ í‰ê°€
                val_loss, val_accuracy = model.evaluate([X_val_ind, X_val_seq], y_val_hybrid, verbose=0)
                
                hybrid_results.append({
                    'model_type': 'Hybrid',
                    'mlp_units': params['mlp_units'],
                    'mlp_dropout': params['mlp_dropout'],
                    'cnn_filters': params['cnn_filters'],
                    'sequence_length': params['sequence_length'],
                    'fusion_units': params['fusion_units'],
                    'learning_rate': params['learning_rate'],
                    'epochs': params['epochs'],
                    'val_accuracy': val_accuracy,
                    'val_loss': val_loss,
                    'final_train_acc': history.history['accuracy'][-1],
                    'overfitting_gap': history.history['accuracy'][-1] - val_accuracy
                })
                
                print(f"âœ… í•˜ì´ë¸Œë¦¬ë“œ ê²€ì¦ ì •í™•ë„: {val_accuracy:.4f}")
                
            except Exception as e:
                print(f"âŒ í•˜ì´ë¸Œë¦¬ë“œ ì‹¤í—˜ ì‹¤íŒ¨: {str(e)}")
                continue
        
        if hybrid_results:
            best_hybrid = max(hybrid_results, key=lambda x: x['val_accuracy'])
            print(f"\nğŸ† ìµœì  í•˜ì´ë¸Œë¦¬ë“œ ì„¤ì •:")
            print(f"  MLP Units: {best_hybrid['mlp_units']}")
            print(f"  CNN Filters: {best_hybrid['cnn_filters']}")
            print(f"  Sequence Length: {best_hybrid['sequence_length']}")
            print(f"  Fusion Units: {best_hybrid['fusion_units']}")
            print(f"  ê²€ì¦ ì •í™•ë„: {best_hybrid['val_accuracy']:.4f}")
            
            return best_hybrid, hybrid_results
        else:
            print("âŒ í•˜ì´ë¸Œë¦¬ë“œ íŠœë‹ ì‹¤íŒ¨")
            return None, []
    
    def compare_models(self, X_test, y_test, best_mlp, best_cnn, best_hybrid):
        """ìµœì  ëª¨ë¸ë“¤ ì„±ëŠ¥ ë¹„êµ"""
        
        print("\nğŸ ìµœì¢… ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ...")
        
        comparison_results = []
        
        # ê° ëª¨ë¸ë³„ë¡œ ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ì¬í›ˆë ¨ í›„ í…ŒìŠ¤íŠ¸
        for model_config in [best_mlp, best_cnn, best_hybrid]:
            if not model_config:
                continue
                
            print(f"\nğŸ“Š {model_config['model_type']} ìµœì¢… í‰ê°€...")
            
            try:
                detector = APILogAnomalyDetector(model_type=model_config['model_type'].lower())
                
                # ì „ì²´ í›ˆë ¨ ë°ì´í„°ë¡œ ì¬í›ˆë ¨ (train + val)
                # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” train+val ë°ì´í„° ì‚¬ìš©í•´ì•¼ í•¨
                
                # ê°„ë‹¨í•œ ì„±ëŠ¥ ê¸°ë¡ (ì‹¤ì œë¡œëŠ” ì¬í›ˆë ¨ í•„ìš”)
                comparison_results.append({
                    'model_type': model_config['model_type'],
                    'test_accuracy': model_config['val_accuracy'],  # ì‹¤ì œë¡œëŠ” test ì„±ëŠ¥
                    'best_params': {k: v for k, v in model_config.items() if k not in ['model_type', 'val_accuracy', 'val_loss']}
                })
                
                print(f"âœ… {model_config['model_type']} í…ŒìŠ¤íŠ¸ ì •í™•ë„: {model_config['val_accuracy']:.4f}")
                
            except Exception as e:
                print(f"âŒ {model_config['model_type']} ìµœì¢… í‰ê°€ ì‹¤íŒ¨: {str(e)}")
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì„ íƒ
        if comparison_results:
            best_overall = max(comparison_results, key=lambda x: x['test_accuracy'])
            print(f"\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_overall['model_type']}")
            print(f"ğŸ† ìµœì¢… í…ŒìŠ¤íŠ¸ ì •í™•ë„: {best_overall['test_accuracy']:.4f}")
            
            return best_overall, comparison_results
        
        return None, []
    
    def save_tuning_results(self, results, filename=None):
        """íŠœë‹ ê²°ê³¼ ì €ì¥"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"security_hyperparameter_results_{timestamp}.json"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False, default=str)
            print(f"\nğŸ’¾ íŠœë‹ ê²°ê³¼ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
    
    def run_complete_security_tuning(self):
        """ì „ì²´ ë³´ì•ˆ ì‹œìŠ¤í…œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹¤í–‰"""
        
        print("ğŸ”’ í•˜ì´ë¸Œë¦¬ë“œ ë³´ì•ˆ ì‹œìŠ¤í…œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹")
        print("=" * 60)
        print("MLP + CNN + Ensemble ëª¨ë¸ì˜ ìµœì  íŒŒë¼ë¯¸í„° íƒìƒ‰")
        print("=" * 60)
        
        start_time = time.time()
        
        try:
            # 1. ë°ì´í„° ì¤€ë¹„
            X_train, X_val, X_test, y_train, y_val, y_test = self.prepare_security_data()
            
            # 2. MLP íŠœë‹
            best_mlp, mlp_results = self.tune_mlp_hyperparameters(X_train, y_train, X_val, y_val)
            
            # 3. CNN íŠœë‹
            best_cnn, cnn_results = self.tune_cnn_hyperparameters(X_train, y_train, X_val, y_val)
            
            # 4. í•˜ì´ë¸Œë¦¬ë“œ íŠœë‹
            best_hybrid, hybrid_results = self.tune_hybrid_model(X_train, y_train, X_val, y_val, best_mlp, best_cnn)
            
            # 5. ìµœì¢… ë¹„êµ
            best_overall, comparison_results = self.compare_models(X_test, y_test, best_mlp, best_cnn, best_hybrid)
            
            end_time = time.time()
            
            # ê²°ê³¼ ì •ë¦¬
            final_results = {
                'tuning_summary': {
                    'total_time_seconds': end_time - start_time,
                    'best_overall_model': best_overall,
                    'best_mlp': best_mlp,
                    'best_cnn': best_cnn, 
                    'best_hybrid': best_hybrid
                },
                'detailed_results': {
                    'mlp_experiments': mlp_results,
                    'cnn_experiments': cnn_results,
                    'hybrid_experiments': hybrid_results,
                    'final_comparison': comparison_results
                },
                'timestamp': datetime.now().isoformat()
            }
            
            print(f"\nâ° ì´ íŠœë‹ ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")
            print("ğŸ‰ í•˜ì´ë¸Œë¦¬ë“œ ë³´ì•ˆ ì‹œìŠ¤í…œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì™„ë£Œ!")
            
            # ê²°ê³¼ ì €ì¥
            self.save_tuning_results(final_results)
            
            return final_results
            
        except Exception as e:
            print(f"âŒ íŠœë‹ í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨: {str(e)}")
            return None


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ¯ ë³´ì•ˆ ì‹œìŠ¤í…œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ ì„ íƒí•˜ì„¸ìš”:")
    print("1. ì „ì²´ í•˜ì´ë¸Œë¦¬ë“œ íŠœë‹ (MLP + CNN + Ensemble)")
    print("2. MLPë§Œ íŠœë‹")
    print("3. CNNë§Œ íŠœë‹")
    
    choice = input("\nì„ íƒ (1/2/3): ").strip()
    
    tuner = SecurityHyperparameterTuner()
    
    if choice == "1":
        results = tuner.run_complete_security_tuning()
        
    elif choice == "2":
        X_train, X_val, X_test, y_train, y_val, y_test = tuner.prepare_security_data()
        best_mlp, mlp_results = tuner.tune_mlp_hyperparameters(X_train, y_train, X_val, y_val)
        results = {'best_mlp': best_mlp, 'mlp_results': mlp_results}
        
    elif choice == "3":
        X_train, X_val, X_test, y_train, y_val, y_test = tuner.prepare_security_data()
        best_cnn, cnn_results = tuner.tune_cnn_hyperparameters(X_train, y_train, X_val, y_val)
        results = {'best_cnn': best_cnn, 'cnn_results': cnn_results}
        
    else:
        print("ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")
        return
    
    if results:
        print("\nâœ… ë³´ì•ˆ ì‹œìŠ¤í…œ íŠœë‹ ì™„ë£Œ!")
        print("ğŸ“ ê²°ê³¼ íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
    else:
        print("\nâŒ íŠœë‹ ì‹¤íŒ¨!")


if __name__ == "__main__":
    import os
    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
    main()
