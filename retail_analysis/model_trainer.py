"""
Online Retail ëª¨ë¸ í›ˆë ¨ ë° í‰ê°€ ëª¨ë“ˆ

ì´ ëª¨ë“ˆì€ Online Retail ë°ì´í„°ë¥¼ í™œìš©í•œ ì„ í˜•íšŒê·€ ëª¨ë¸ì˜ 
í›ˆë ¨, ê²€ì¦, í‰ê°€ë¥¼ ë‹´ë‹¹í•©ë‹ˆë‹¤.
"""

import pandas as pd
import numpy as np
import warnings
from typing import Dict, List, Optional, Tuple
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from scipy import stats

warnings.filterwarnings("ignore")


class RetailModelTrainer:
    """
    Online Retail ëª¨ë¸ í›ˆë ¨ ë° í‰ê°€ë¥¼ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤
    
    ì„ í˜•íšŒê·€ ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ì˜ˆì¸¡ ëª¨ë¸ë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    
    def __init__(self):
        """ì´ˆê¸°í™” ë©”ì„œë“œ"""
        self.model = None
        self.scaler = None
        self.feature_names = None
        self.training_results = None
        self.evaluation_results = None
        
    def prepare_modeling_data(self, customer_features: pd.DataFrame, target_column: str = 'predicted_next_amount') -> Tuple[pd.DataFrame, pd.Series]:
        """ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë§ì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„"""
        print("âš™ï¸ ëª¨ë¸ë§ ë°ì´í„° ì¤€ë¹„ ì¤‘...")
        
        df = customer_features.copy()
        
        # íƒ€ê²Ÿ ë³€ìˆ˜ í™•ì¸
        if target_column not in df.columns:
            raise ValueError(f"íƒ€ê²Ÿ ë³€ìˆ˜ '{target_column}'ì´ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤.")
        
        # íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„ë¦¬
        y = df[target_column].copy()
        
        # ëª¨ë¸ë§ì— ì‚¬ìš©í•  íŠ¹ì„± ì„ íƒ
        exclude_cols = [
            target_column, 'customer_value_category', 'monthly_avg_amount',
            'first_purchase', 'last_purchase', 'customer_segment'
        ]
        
        feature_cols = [col for col in df.columns if col not in exclude_cols]
        X = df[feature_cols].copy()
        
        # ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©
        categorical_cols = X.select_dtypes(include=['object']).columns
        if len(categorical_cols) > 0:
            print(f"   ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©: {list(categorical_cols)}")
            X = pd.get_dummies(X, columns=categorical_cols, prefix=categorical_cols)
        
        # ë¬´í•œê°’ ë° ê²°ì¸¡ê°’ ì²˜ë¦¬
        X = X.replace([np.inf, -np.inf], np.nan)
        X = X.fillna(X.median())
        
        # íŠ¹ì„±ëª… ì €ì¥
        self.feature_names = X.columns.tolist()
        
        print(f"âœ… ëª¨ë¸ë§ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ")
        print(f"   íŠ¹ì„± ìˆ˜: {X.shape[1]}ê°œ")
        print(f"   ìƒ˜í”Œ ìˆ˜: {X.shape[0]}ê°œ")
        print(f"   íƒ€ê²Ÿ ë³€ìˆ˜: {target_column}")
        
        return X, y
    
    def train_model(self, X: pd.DataFrame, y: pd.Series, 
                   test_size: float = 0.2, 
                   scale_features: bool = True,
                   random_state: int = 42) -> Dict:
        """ì„ í˜•íšŒê·€ ëª¨ë¸ í›ˆë ¨"""
        print("ğŸš€ ì„ í˜•íšŒê·€ ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")
        
        # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=random_state
        )
        
        # íŠ¹ì„± ì •ê·œí™”
        if scale_features:
            print("   íŠ¹ì„± ì •ê·œí™” ìˆ˜í–‰ ì¤‘...")
            self.scaler = StandardScaler()
            X_train_scaled = self.scaler.fit_transform(X_train)
            X_test_scaled = self.scaler.transform(X_test)
            
            # DataFrame í˜•íƒœë¡œ ë³€í™˜
            X_train_final = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)
            X_test_final = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)
        else:
            X_train_final = X_train.copy()
            X_test_final = X_test.copy()
        
        # ëª¨ë¸ í›ˆë ¨
        print("   ì„ í˜•íšŒê·€ ëª¨ë¸ í›ˆë ¨ ì¤‘...")
        self.model = LinearRegression()
        self.model.fit(X_train_final, y_train)
        
        # ì˜ˆì¸¡
        y_train_pred = self.model.predict(X_train_final)
        y_test_pred = self.model.predict(X_test_final)
        
        # í›ˆë ¨ ê²°ê³¼ ì €ì¥
        self.training_results = {
            'model': self.model,
            'scaler': self.scaler,
            'X_train': X_train_final,
            'X_test': X_test_final,
            'y_train': y_train,
            'y_test': y_test,
            'y_train_pred': y_train_pred,
            'y_test_pred': y_test_pred,
            'feature_names': self.feature_names,
            'test_size': test_size,
            'scale_features': scale_features,
            'random_state': random_state
        }
        
        print("âœ… ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ")
        
        return self.training_results
    
    def evaluate_model(self) -> Dict:
        """ëª¨ë¸ ì„±ëŠ¥ í‰ê°€"""
        if self.training_results is None:
            raise ValueError("ëª¨ë¸ì´ ì•„ì§ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        print("ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì¤‘...")
        
        # í›ˆë ¨ ê²°ê³¼ì—ì„œ ë°ì´í„° ì¶”ì¶œ
        y_train = self.training_results['y_train']
        y_test = self.training_results['y_test']
        y_train_pred = self.training_results['y_train_pred']
        y_test_pred = self.training_results['y_test_pred']
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
        evaluation_metrics = {
            # í›ˆë ¨ ì„±ëŠ¥
            'r2_train': r2_score(y_train, y_train_pred),
            'mae_train': mean_absolute_error(y_train, y_train_pred),
            'rmse_train': np.sqrt(mean_squared_error(y_train, y_train_pred)),
            
            # í…ŒìŠ¤íŠ¸ ì„±ëŠ¥
            'r2_test': r2_score(y_test, y_test_pred),
            'mae_test': mean_absolute_error(y_test, y_test_pred),
            'rmse_test': np.sqrt(mean_squared_error(y_test, y_test_pred)),
            
            # ê³¼ì í•© ë¶„ì„
            'performance_gap': abs(r2_score(y_test, y_test_pred) - r2_score(y_train, y_train_pred)),
            'relative_error': (mean_absolute_error(y_test, y_test_pred) / y_test.mean()) * 100,
            
            # ì”ì°¨ ë¶„ì„
            'residuals': y_test - y_test_pred,
            'residuals_std': np.std(y_test - y_test_pred),
            'residuals_mean': np.mean(y_test - y_test_pred)
        }
        
        # ì •ê·œì„± ê²€ì •
        residuals = y_test - y_test_pred
        shapiro_stat, shapiro_p = stats.shapiro(residuals[:min(5000, len(residuals))])  # ìƒ˜í”Œ í¬ê¸° ì œí•œ
        
        evaluation_metrics['normality_test'] = {
            'shapiro_stat': shapiro_stat,
            'shapiro_p_value': shapiro_p,
            'is_normal': shapiro_p > 0.05
        }
        
        # ì´ë¶„ì‚°ì„± ê²€ì • (Breusch-Pagan test ê°„ì†Œí™” ë²„ì „)
        squared_residuals = residuals ** 2
        X_test = self.training_results['X_test']
        
        # ê°„ë‹¨í•œ ìƒê´€ê´€ê³„ ê¸°ë°˜ ì´ë¶„ì‚°ì„± ê²€ì •
        heteroscedasticity_correlation = np.corrcoef(y_test_pred, squared_residuals)[0, 1]
        
        evaluation_metrics['heteroscedasticity_test'] = {
            'correlation': heteroscedasticity_correlation,
            'is_homoscedastic': abs(heteroscedasticity_correlation) < 0.1
        }
        
        # íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„
        feature_importance = self._analyze_feature_importance()
        evaluation_metrics['feature_importance'] = feature_importance
        
        # ëª¨ë¸ í•´ì„
        model_interpretation = self._interpret_model_performance(evaluation_metrics)
        evaluation_metrics['interpretation'] = model_interpretation
        
        self.evaluation_results = evaluation_metrics
        
        print("âœ… ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì™„ë£Œ")
        
        return evaluation_metrics
    
    def _analyze_feature_importance(self) -> Dict:
        """íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„"""
        if self.model is None or self.feature_names is None:
            return {"error": "ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
        
        # íšŒê·€ ê³„ìˆ˜ ê¸°ë°˜ ì¤‘ìš”ë„
        coefficients = self.model.coef_
        
        feature_importance = pd.DataFrame({
            'feature': self.feature_names,
            'coefficient': coefficients,
            'abs_coefficient': np.abs(coefficients)
        }).sort_values('abs_coefficient', ascending=False)
        
        # ìƒìœ„ 10ê°œ íŠ¹ì„±
        top_features = feature_importance.head(10).to_dict('records')
        
        # ì–‘ì˜ ì˜í–¥ê³¼ ìŒì˜ ì˜í–¥ íŠ¹ì„± ë¶„ë¦¬
        positive_impact = feature_importance[feature_importance['coefficient'] > 0].head(5)
        negative_impact = feature_importance[feature_importance['coefficient'] < 0].head(5)
        
        return {
            'top_10_features': top_features,
            'positive_impact_features': positive_impact.to_dict('records'),
            'negative_impact_features': negative_impact.to_dict('records'),
            'intercept': self.model.intercept_,
            'total_features': len(self.feature_names)
        }
    
    def _interpret_model_performance(self, metrics: Dict) -> Dict:
        """ëª¨ë¸ ì„±ëŠ¥ í•´ì„"""
        
        interpretation = {
            'overall_performance': '',
            'overfitting_status': '',
            'prediction_accuracy': '',
            'business_applicability': '',
            'improvement_suggestions': []
        }
        
        # ì „ì²´ ì„±ëŠ¥ í‰ê°€
        test_r2 = metrics['r2_test']
        if test_r2 >= 0.8:
            interpretation['overall_performance'] = "ìš°ìˆ˜í•œ ì„±ëŠ¥ (RÂ² â‰¥ 0.8)"
        elif test_r2 >= 0.6:
            interpretation['overall_performance'] = "ì–‘í˜¸í•œ ì„±ëŠ¥ (RÂ² â‰¥ 0.6)"
        elif test_r2 >= 0.4:
            interpretation['overall_performance'] = "ë³´í†µ ì„±ëŠ¥ (RÂ² â‰¥ 0.4)"
        else:
            interpretation['overall_performance'] = "ê°œì„  í•„ìš” (RÂ² < 0.4)"
        
        # ê³¼ì í•© ë¶„ì„
        performance_gap = metrics['performance_gap']
        if performance_gap <= 0.05:
            interpretation['overfitting_status'] = "ê³¼ì í•© ì—†ìŒ"
        elif performance_gap <= 0.1:
            interpretation['overfitting_status'] = "ê²½ë¯¸í•œ ê³¼ì í•©"
        else:
            interpretation['overfitting_status'] = "ê³¼ì í•© ë°œìƒ"
            interpretation['improvement_suggestions'].append("ì •ê·œí™” ê¸°ë²• ì ìš© ê³ ë ¤")
        
        # ì˜ˆì¸¡ ì •í™•ë„ ë¶„ì„
        relative_error = metrics['relative_error']
        if relative_error <= 15:
            interpretation['prediction_accuracy'] = "ê³ ì •ë°€ë„ ì˜ˆì¸¡ (ìƒëŒ€ì˜¤ì°¨ â‰¤ 15%)"
            interpretation['business_applicability'] = "ê°œì¸í™” ë§ˆì¼€íŒ… ì „ëµ ìˆ˜ë¦½ ê°€ëŠ¥"
        elif relative_error <= 25:
            interpretation['prediction_accuracy'] = "ì¤‘ê°„ ì •ë°€ë„ ì˜ˆì¸¡ (ìƒëŒ€ì˜¤ì°¨ â‰¤ 25%)"
            interpretation['business_applicability'] = "ì„¸ê·¸ë¨¼íŠ¸ë³„ ì „ëµ ìˆ˜ë¦½ ì í•©"
        else:
            interpretation['prediction_accuracy'] = "ë‚®ì€ ì •ë°€ë„ ì˜ˆì¸¡ (ìƒëŒ€ì˜¤ì°¨ > 25%)"
            interpretation['business_applicability'] = "ì „ë°˜ì  íŠ¸ë Œë“œ íŒŒì•… ìˆ˜ì¤€"
            interpretation['improvement_suggestions'].append("ì¶”ê°€ íŠ¹ì„± ê³µí•™ í•„ìš”")
        
        # ê°œì„  ì œì•ˆ
        if test_r2 < 0.6:
            interpretation['improvement_suggestions'].append("ë¹„ì„ í˜• ëª¨ë¸ ê³ ë ¤")
        
        if not metrics['normality_test']['is_normal']:
            interpretation['improvement_suggestions'].append("ì”ì°¨ ì •ê·œì„± ê°œì„  í•„ìš”")
        
        if not metrics['heteroscedasticity_test']['is_homoscedastic']:
            interpretation['improvement_suggestions'].append("ë“±ë¶„ì‚°ì„± ê°€ì • ìœ„ë°˜, ê°€ì¤‘íšŒê·€ ê³ ë ¤")
        
        return interpretation
    
    def predict_customer_value(self, customer_data: pd.DataFrame) -> pd.DataFrame:
        """ìƒˆë¡œìš´ ê³ ê° ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡"""
        if self.model is None:
            raise ValueError("ëª¨ë¸ì´ ì•„ì§ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # íŠ¹ì„± ì „ì²˜ë¦¬
        X = customer_data[self.feature_names].copy()
        
        # ê²°ì¸¡ê°’ ì²˜ë¦¬
        X = X.fillna(X.median())
        
        # ì •ê·œí™” ì ìš©
        if self.scaler is not None:
            X_scaled = self.scaler.transform(X)
            X = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)
        
        # ì˜ˆì¸¡
        predictions = self.model.predict(X)
        
        # ê²°ê³¼ DataFrame ìƒì„±
        result = customer_data.copy()
        result['predicted_value'] = predictions
        result['prediction_confidence'] = 'High' if self.evaluation_results['r2_test'] >= 0.6 else 'Medium'
        
        return result
    
    def get_model_summary(self) -> Dict:
        """ëª¨ë¸ ìš”ì•½ ì •ë³´ ë°˜í™˜"""
        if self.training_results is None or self.evaluation_results is None:
            return {"error": "ëª¨ë¸ì´ í›ˆë ¨ë˜ê±°ë‚˜ í‰ê°€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
        
        summary = {
            'model_type': 'Linear Regression',
            'training_samples': len(self.training_results['y_train']),
            'test_samples': len(self.training_results['y_test']),
            'features_count': len(self.feature_names),
            'performance_metrics': {
                'r2_score': round(self.evaluation_results['r2_test'], 4),
                'mae': round(self.evaluation_results['mae_test'], 2),
                'rmse': round(self.evaluation_results['rmse_test'], 2),
                'relative_error_pct': round(self.evaluation_results['relative_error'], 2)
            },
            'model_quality': {
                'overfitting_risk': 'Low' if self.evaluation_results['performance_gap'] <= 0.05 else 'Medium',
                'residuals_normal': self.evaluation_results['normality_test']['is_normal'],
                'homoscedastic': self.evaluation_results['heteroscedasticity_test']['is_homoscedastic']
            },
            'business_impact': self.evaluation_results['interpretation']['business_applicability'],
            'top_features': [f['feature'] for f in self.evaluation_results['feature_importance']['top_10_features'][:5]]
        }
        
        return summary
