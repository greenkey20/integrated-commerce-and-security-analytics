# ьїїВЮ╝вфЁ: tests/integration/memory_stress_test.py

import sys
import os

# ьћёвАюВаЮьіИ вБеьіИ вћћваЅьєавдгвЦ╝ Python Ж▓йвАюВЌљ ВХћЖ░ђ
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
sys.path.insert(0, project_root)

import psutil
import pandas as pd
import numpy as np
import time
import gc
from data.loaders.cicids_working_files_loader import WorkingCICIDSLoader


def get_memory_usage():
    """ьўёВъг вЕћвфевдг ВѓгВџЕвЪЅ в░ўьЎў (MB)"""
    process = psutil.Process()
    memory_info = process.memory_info()
    return memory_info.rss / 1024 / 1024  # MB вІеВюё


def test_memory_scaling():
    """ВаљВДёВаЂВю╝вАю ВЃўьћї ВѕўвЦ╝ віўвацЖ░ђвЕ░ вЕћвфевдг ьЁїВіцьіИ"""

    print("­ЪДф вЕћвфевдг ВѓгВџЕвЪЅ ьЁїВіцьіИ ВІюВъЉ")
    print("=" * 50)

    # ВІюВіцьЁю вЕћвфевдг ВаЋв│┤
    total_memory = psutil.virtual_memory().total / 1024 / 1024 / 1024  # GB
    available_memory = psutil.virtual_memory().available / 1024 / 1024 / 1024

    print(f"­Ъњ╗ ВІюВіцьЁю В┤Ю вЕћвфевдг: {total_memory:.1f} GB")
    print(f"­Ъњ╗ ВѓгВџЕ Ж░ђвіЦ вЕћвфевдг: {available_memory:.1f} GB")
    print()

    # ьЁїВіцьіИ ВЃўьћї ВѕўвЊц (ВаљВДёВаЂ ВдЮЖ░ђ)
    test_samples = [50000, 100000, 200000, 300000, 500000, 1000000]

    # ьћёвАюВаЮьіИ вБеьіИ ЖИ░ВцђВю╝вАю Ж▓йвАю ВёцВаЋ
    data_dir = os.path.join(project_root, "data", "cicids2017")
    loader = WorkingCICIDSLoader(data_dir)

    results = []

    for target_samples in test_samples:
        print(f"­ЪћЇ ьЁїВіцьіИ: {target_samples:,}Ж░ю ВЃўьћї")

        try:
            # ВІюВъЉ вЕћвфевдг ВИАВаЋ
            start_memory = get_memory_usage()
            start_time = time.time()

            # вЇ░ВЮ┤ьё░ вАювЊю
            dataset = loader.load_working_files(target_samples=target_samples)

            # вАювЊю ьЏё вЕћвфевдг ВИАВаЋ
            load_memory = get_memory_usage()
            load_time = time.time() - start_time

            # вЇ░ВЮ┤ьё░ ВаЋв│┤
            data_size_mb = dataset.memory_usage(deep=True).sum() / 1024 / 1024

            # Ж▓░Ж│╝ ЖИ░вАЮ
            result = {
                'samples': target_samples,
                'load_time': load_time,
                'start_memory_mb': start_memory,
                'peak_memory_mb': load_memory,
                'memory_increase_mb': load_memory - start_memory,
                'data_size_mb': data_size_mb,
                'success': True
            }

            results.append(result)

            print(f"   РюЁ Вё▒Ж│х!")
            print(f"   РЈ▒№ИЈ вАювћЕ ВІюЖ░ё: {load_time:.1f}В┤ѕ")
            print(f"   ­ЪЊі вЇ░ВЮ┤ьё░ ьЂгЖИ░: {data_size_mb:.1f} MB")
            print(f"   ­ЪњЙ вЕћвфевдг ВдЮЖ░ђ: {load_memory - start_memory:.1f} MB")
            print(f"   ­ЪњЙ В┤Ю вЕћвфевдг ВѓгВџЕ: {load_memory:.1f} MB")

            # вЕћвфевдг ВаЋвдг
            del dataset
            gc.collect()
            time.sleep(2)  # вЕћвфевдг ВаЋвдг вїђЖИ░

            # ВЋѕВаё В▓┤ьЂг: вЕћвфевдг ВѓгВџЕвЦаВЮ┤ 80% вёўВю╝вЕ┤ ВцЉвІе
            current_usage = psutil.virtual_memory().percent
            if current_usage > 80:
                print(f"   Рџа№ИЈ вЕћвфевдг ВѓгВџЕвЦа {current_usage:.1f}% - ВЋѕВаёВЮё ВюёьЋ┤ ьЁїВіцьіИ ВцЉвІе")
                break

        except MemoryError:
            print(f"   РЮї вЕћвфевдг вХђВА▒!")
            result = {
                'samples': target_samples,
                'success': False,
                'error': 'MemoryError'
            }
            results.append(result)
            break

        except Exception as e:
            print(f"   РЮї ВўцвЦў: {str(e)[:50]}...")
            result = {
                'samples': target_samples,
                'success': False,
                'error': str(e)[:100]
            }
            results.append(result)

        print()

    # Ж▓░Ж│╝ ВџћВЋй
    print("­ЪЊІ вЕћвфевдг ьЁїВіцьіИ Ж▓░Ж│╝ ВџћВЋй")
    print("=" * 50)

    successful_tests = [r for r in results if r.get('success', False)]

    if successful_tests:
        max_successful = max(successful_tests, key=lambda x: x['samples'])
        print(f"РюЁ Вхювїђ Вё▒Ж│х ВЃўьћї Вѕў: {max_successful['samples']:,}Ж░ю")
        print(f"РЈ▒№ИЈ Вхювїђ вАювћЕ ВІюЖ░ё: {max_successful['load_time']:.1f}В┤ѕ")
        print(f"­ЪњЙ Вхювїђ вЕћвфевдг ВѓгВџЕ: {max_successful['peak_memory_mb']:.1f} MB")

        # ВХћВ▓ю ВЃўьћї Вѕў (ВЋѕВаё вДѕВДё Ж│авац)
        recommended_samples = max_successful['samples'] * 0.8  # 80% ВЋѕВаё вДѕВДё
        print(f"­ЪњА ЖХїВъЦ ВЃўьћї Вѕў: {recommended_samples:,.0f}Ж░ю (ВЋѕВаё вДѕВДё ьЈгьЋе)")

    else:
        print("РЮї вфевЊа ьЁїВіцьіИ ВІцьїе")

    return results


def test_model_training_memory():
    """вфевЇИ ьЏѕвае ВІю вЕћвфевдг ВѓгВџЕвЪЅ ьЁїВіцьіИ"""
    print("\n­Ъцќ вфевЇИ ьЏѕвае вЕћвфевдг ьЁїВіцьіИ")
    print("=" * 30)

    # ВцЉЖ░ё ьЂгЖИ░ вЇ░ВЮ┤ьё░вАю ьЏѕвае вЕћвфевдг ьЁїВіцьіИ
    data_dir = os.path.join(project_root, "data", "cicids2017")
    loader = WorkingCICIDSLoader(data_dir)

    # 10вДї Ж░ю ВЃўьћївАю ьЁїВіцьіИ
    dataset = loader.load_working_files(target_samples=100000)

    print(f"­ЪЊі ьЏѕвае вЇ░ВЮ┤ьё░: {len(dataset):,}Ж░ю")

    # вЕћвфевдг ВИАВаЋ
    before_training = get_memory_usage()

    # Ж░ёвІеьЋю ВаёВ▓ўвдг (ВІцВаю ьЏѕвае ВІюв«гваѕВЮ┤ВЁў)
    print("­Ъћё ВаёВ▓ўвдг ВцЉ...")

    # ВѕўВ╣ў В╗гвЪ╝вДї ВёаьЃЮ (Label ВаюВЎИ)
    numeric_columns = dataset.select_dtypes(include=[np.number]).columns
    X = dataset[numeric_columns].fillna(0)  # Ж▓░ВИАВ╣ў В▓ўвдг

    after_preprocessing = get_memory_usage()

    print(f"­ЪњЙ ВаёВ▓ўвдг ьЏё вЕћвфевдг: {after_preprocessing - before_training:.1f} MB ВдЮЖ░ђ")

    # вфевЇИ ьЏѕвае ВІюв«гваѕВЮ┤ВЁў (ВІцВаю TensorFlow ВЌєВЮ┤)
    print("­ЪДа вфевЇИ ьЏѕвае ВІюв«гваѕВЮ┤ВЁў...")

    # в░░В╣ў В▓ўвдг ВІюв«гваѕВЮ┤ВЁў
    batch_size = 1000
    batches = len(X) // batch_size

    max_memory = before_training

    for i in range(min(10, batches)):  # В▓ўВЮї 10Ж░ю в░░В╣ўвДї ьЁїВіцьіИ
        batch_data = X.iloc[i * batch_size:(i + 1) * batch_size]
        current_memory = get_memory_usage()
        max_memory = max(max_memory, current_memory)

        if i % 5 == 0:
            print(f"   в░░В╣ў {i + 1}: {current_memory:.1f} MB")

    print(f"­ЪњЙ ьЏѕвае ВІю Вхювїђ вЕћвфевдг: {max_memory:.1f} MB")
    print(f"­ЪњЙ В┤Ю вЕћвфевдг ВдЮЖ░ђ: {max_memory - before_training:.1f} MB")

    # ВаЋвдг
    del dataset, X
    gc.collect()


if __name__ == "__main__":
    print("­Ъџђ CICIDS2017 вЕћвфевдг ВѓгВџЕвЪЅ ВбЁьЋЕ ьЁїВіцьіИ")
    print(f"­ЪЊЇ ьћёвАюВаЮьіИ вБеьіИ: {project_root}")
    print("=" * 60)

    # 1. вЇ░ВЮ┤ьё░ вАювћЕ вЕћвфевдг ьЁїВіцьіИ
    loading_results = test_memory_scaling()

    # 2. вфевЇИ ьЏѕвае вЕћвфевдг ьЁїВіцьіИ
    test_model_training_memory()

    print("\n­Ъј» ВхюВбЁ ЖХїВъЦВѓгьЋГ:")
    successful_tests = [r for r in loading_results if r.get('success', False)]

    if successful_tests:
        max_samples = max(successful_tests, key=lambda x: x['samples'])['samples']

        if max_samples >= 500000:
            print("РюЁ 50вДї Ж░ю ВЃўьћї В▓ўвдг Ж░ђвіЦ - вїђВџЕвЪЅ ьЏѕвае ЖХїВъЦ")
        elif max_samples >= 200000:
            print("РюЁ 20вДї Ж░ю ВЃўьћї В▓ўвдг Ж░ђвіЦ - ВцЉвїђВџЕвЪЅ ьЏѕвае ЖХїВъЦ")
        elif max_samples >= 100000:
            print("РџА 10вДї Ж░ю ВЃўьћї В▓ўвдг Ж░ђвіЦ - ВаЂВаЋ Жиювфе ьЏѕвае ЖХїВъЦ")
        else:
            print("Рџа№ИЈ ВІюв«гваѕВЮ┤ВЁў вЇ░ВЮ┤ьё░ ВѓгВџЕ ЖХїВъЦ")

    print("\n­ЪњА вЕћвфевдгЖ░ђ вХђВА▒ьЋўвЕ┤ в░░В╣ў В▓ўвдгвѓў ВЃўьћївДЂВЮё Ж│авацьЋўВёИВџћ!")