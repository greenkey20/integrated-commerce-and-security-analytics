"""
ë³´ì•ˆ ëª¨ë¸ ì„±ëŠ¥ ì´ìƒ ì§„ë‹¨ ë„êµ¬

ì •í™•ë„ 1.0ì´ ë‚˜ì˜¤ëŠ” ì›ì¸ì„ ë¶„ì„í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
from data.loaders.unified_security_loader import UnifiedSecurityLoader

def diagnose_perfect_performance():
    """ì™„ë²½í•œ ì„±ëŠ¥ì˜ ì›ì¸ ë¶„ì„"""
    print("=== ë³´ì•ˆ ëª¨ë¸ ì„±ëŠ¥ ì´ìƒ ì§„ë‹¨ ===")
    
    # 1. ìƒ˜í”Œ ë°ì´í„° ìƒì„±
    loader = UnifiedSecurityLoader()
    data = loader.generate_sample_data(total_samples=1000, attack_ratio=0.6)
    
    print(f"ë°ì´í„° í¬ê¸°: {data.shape}")
    print(f"ë¼ë²¨ ë¶„í¬:\n{data['Label'].value_counts()}")
    
    # 2. íŠ¹ì„±ë³„ ë¶„ë¦¬ ì •ë„ ë¶„ì„
    analyze_feature_separation(data)
    
    # 3. í´ëŸ¬ìŠ¤í„° ë¶„ì„
    analyze_clustering_patterns(data)
    
    # 4. ë°ì´í„° ë³µì¡ì„± ì¸¡ì •
    measure_data_complexity(data)
    
    # 5. ê°œì„  ë°©ì•ˆ ì œì•ˆ
    suggest_improvements()

def analyze_feature_separation(data):
    """íŠ¹ì„±ë³„ ë¶„ë¦¬ ì •ë„ ë¶„ì„"""
    print("\n=== íŠ¹ì„±ë³„ ë¶„ë¦¬ ì •ë„ ë¶„ì„ ===")
    
    numeric_features = data.select_dtypes(include=[np.number]).columns[:5]
    
    separation_scores = []
    for feature in numeric_features:
        normal_data = data[data['Label'] == 'BENIGN'][feature]
        attack_data = data[data['Label'] != 'BENIGN'][feature]
        
        # í‰ê·  ì°¨ì´ ë¹„ìœ¨
        if normal_data.mean() != 0:
            ratio = abs(attack_data.mean() / normal_data.mean())
        else:
            ratio = float('inf')
        
        # ë¶„í¬ ê²¹ì¹¨ ì •ë„ (Wasserstein distance ê·¼ì‚¬)
        overlap = calculate_distribution_overlap(normal_data, attack_data)
        
        separation_scores.append({
            'feature': feature,
            'mean_ratio': ratio,
            'overlap': overlap,
            'normal_mean': normal_data.mean(),
            'attack_mean': attack_data.mean(),
            'normal_std': normal_data.std(),
            'attack_std': attack_data.std()
        })
        
        print(f"{feature}:")
        print(f"  ì •ìƒ í‰ê· : {normal_data.mean():.2f} Â± {normal_data.std():.2f}")
        print(f"  ê³µê²© í‰ê· : {attack_data.mean():.2f} Â± {attack_data.std():.2f}")
        print(f"  ë¹„ìœ¨: {ratio:.2f}x, ê²¹ì¹¨: {overlap:.3f}")
    
    # ê°€ì¥ ë¶„ë¦¬ê°€ ì˜ ë˜ëŠ” íŠ¹ì„±ë“¤
    separation_df = pd.DataFrame(separation_scores)
    separation_df = separation_df.sort_values('mean_ratio', ascending=False)
    
    print("\nê°€ì¥ ë¶„ë¦¬ê°€ ì˜ ë˜ëŠ” íŠ¹ì„±ë“¤:")
    print(separation_df[['feature', 'mean_ratio', 'overlap']].head())
    
    return separation_df

def calculate_distribution_overlap(dist1, dist2):
    """ë‘ ë¶„í¬ì˜ ê²¹ì¹¨ ì •ë„ ê³„ì‚° (0: ì™„ì „ ë¶„ë¦¬, 1: ì™„ì „ ê²¹ì¹¨)"""
    try:
        # íˆìŠ¤í† ê·¸ë¨ ê¸°ë°˜ ê²¹ì¹¨ ê³„ì‚°
        min_val = min(dist1.min(), dist2.min())
        max_val = max(dist1.max(), dist2.max())
        
        bins = np.linspace(min_val, max_val, 50)
        hist1, _ = np.histogram(dist1, bins=bins, density=True)
        hist2, _ = np.histogram(dist2, bins=bins, density=True)
        
        # ê²¹ì¹˜ëŠ” ì˜ì—­ ê³„ì‚°
        overlap = np.sum(np.minimum(hist1, hist2)) / np.sum(np.maximum(hist1, hist2))
        return overlap
    except:
        return 0.0

def analyze_clustering_patterns(data):
    """í´ëŸ¬ìŠ¤í„°ë§ íŒ¨í„´ ë¶„ì„"""
    print("\n=== í´ëŸ¬ìŠ¤í„°ë§ íŒ¨í„´ ë¶„ì„ ===")
    
    # ìˆ˜ì¹˜í˜• íŠ¹ì„±ë§Œ ì„ íƒ
    numeric_features = data.select_dtypes(include=[np.number]).columns
    X = data[numeric_features].values
    
    # PCAë¡œ ì°¨ì› ì¶•ì†Œ
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(X)
    
    # ë¼ë²¨ë³„ ë¶„ë¦¬ë„ ì¸¡ì •
    labels = data['Label'].values
    unique_labels = np.unique(labels)
    
    print("PCA ì„¤ëª… ê°€ëŠ¥í•œ ë¶„ì‚°:")
    print(f"PC1: {pca.explained_variance_ratio_[0]:.3f}")
    print(f"PC2: {pca.explained_variance_ratio_[1]:.3f}")
    print(f"ì´í•©: {pca.explained_variance_ratio_.sum():.3f}")
    
    # í´ëŸ¬ìŠ¤í„° ê°„ ê±°ë¦¬ ì¸¡ì •
    cluster_distances = {}
    for label in unique_labels:
        mask = labels == label
        centroid = X_pca[mask].mean(axis=0)
        cluster_distances[label] = centroid
    
    # í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ ê°„ ê±°ë¦¬
    distance_matrix = {}
    for label1 in unique_labels:
        for label2 in unique_labels:
            if label1 != label2:
                dist = np.linalg.norm(
                    cluster_distances[label1] - cluster_distances[label2]
                )
                distance_matrix[f"{label1} vs {label2}"] = dist
    
    print("\ní´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ ê°„ ê±°ë¦¬:")
    for pair, distance in distance_matrix.items():
        print(f"{pair}: {distance:.3f}")
    
    return X_pca, cluster_distances

def measure_data_complexity(data):
    """ë°ì´í„° ë³µì¡ì„± ì¸¡ì •"""
    print("\n=== ë°ì´í„° ë³µì¡ì„± ì¸¡ì • ===")
    
    # 1. í´ë˜ìŠ¤ ë¶ˆê· í˜•
    label_counts = data['Label'].value_counts()
    imbalance_ratio = label_counts.max() / label_counts.min()
    print(f"í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¹„ìœ¨: {imbalance_ratio:.2f}")
    
    # 2. íŠ¹ì„± ê°„ ìƒê´€ê´€ê³„
    numeric_data = data.select_dtypes(include=[np.number])
    correlation_matrix = numeric_data.corr().abs()
    high_corr_pairs = (correlation_matrix > 0.9).sum().sum() - len(correlation_matrix)
    print(f"ë†’ì€ ìƒê´€ê´€ê³„ íŠ¹ì„± ìŒ ìˆ˜ (>0.9): {high_corr_pairs}")
    
    # 3. íŠ¹ì„± ë¶„ì‚°
    feature_variances = numeric_data.var()
    low_variance_features = (feature_variances < 0.01).sum()
    print(f"ë‚®ì€ ë¶„ì‚° íŠ¹ì„± ìˆ˜ (<0.01): {low_variance_features}")
    
    # 4. ì´ìƒì¹˜ ë¹„ìœ¨
    outlier_counts = []
    for column in numeric_data.columns:
        Q1 = numeric_data[column].quantile(0.25)
        Q3 = numeric_data[column].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        outliers = (numeric_data[column] < lower_bound) | (numeric_data[column] > upper_bound)
        outlier_counts.append(outliers.sum())
    
    total_outliers = sum(outlier_counts)
    outlier_ratio = total_outliers / (len(data) * len(numeric_data.columns))
    print(f"ì´ìƒì¹˜ ë¹„ìœ¨: {outlier_ratio:.3f}")
    
    return {
        'imbalance_ratio': imbalance_ratio,
        'high_corr_pairs': high_corr_pairs,
        'low_variance_features': low_variance_features,
        'outlier_ratio': outlier_ratio
    }

def suggest_improvements():
    """ê°œì„  ë°©ì•ˆ ì œì•ˆ"""
    print("\n=== ğŸš€ ê°œì„  ë°©ì•ˆ ì œì•ˆ ===")
    
    improvements = [
        "1. ë” í˜„ì‹¤ì ì¸ ë°ì´í„° ìƒì„±:",
        "   - ì •ìƒê³¼ ê³µê²© íŒ¨í„´ ê°„ ê²¹ì¹˜ëŠ” ì˜ì—­ ì¶”ê°€",
        "   - ë…¸ì´ì¦ˆì™€ ë³€ë™ì„± ì¦ê°€",
        "   - ë‹¤ì–‘í•œ ê³µê²© ê°•ë„ ëª¨ë¸ë§",
        "",
        "2. ë°ì´í„° í’ˆì§ˆ ê°œì„ :",
        "   - ìƒ˜í”Œ ìˆ˜ ì¦ê°€ (10K â†’ 100K)",
        "   - ë” ë³µì¡í•œ íŠ¹ì„± ê´€ê³„ ëª¨ë¸ë§",
        "   - ì‹¤ì œ CICIDS2017 ë°ì´í„° í™œìš©",
        "",
        "3. ëª¨ë¸ ê²€ì¦ ê°•í™”:",
        "   - êµì°¨ê²€ì¦ ì¶”ê°€",
        "   - í™€ë“œì•„ì›ƒ ê²€ì¦ì…‹ ë¶„ë¦¬",
        "   - ë‹¤ì–‘í•œ ë©”íŠ¸ë¦­ í™œìš© (AUC, PR-AUC)",
        "",
        "4. ì •ì¹™í™” ë° ë³µì¡ì„± ì œì–´:",
        "   - Dropout ë¹„ìœ¨ ì¦ê°€",
        "   - L1/L2 ì •ê·œí™” ì¶”ê°€",
        "   - ë” ì‘ì€ ëª¨ë¸ ì•„í‚¤í…ì²˜ ì‹œë„"
    ]
    
    for improvement in improvements:
        print(improvement)

def create_realistic_sample_data(n_samples=10000):
    """ë” í˜„ì‹¤ì ì¸ ìƒ˜í”Œ ë°ì´í„° ìƒì„±"""
    print("\n=== ê°œì„ ëœ í˜„ì‹¤ì  ë°ì´í„° ìƒì„± ===")
    
    # ë” ë³µì¡í•˜ê³  í˜„ì‹¤ì ì¸ íŒ¨í„´ìœ¼ë¡œ ë°ì´í„° ìƒì„±
    loader = UnifiedSecurityLoader()
    
    # ì›ë³¸ generate_sample_dataë¥¼ ì˜¤ë²„ë¼ì´ë“œí•˜ëŠ” ë°©ë²•
    # (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” í´ë˜ìŠ¤ë¥¼ ìƒì†ë°›ì•„ ë©”ì„œë“œë¥¼ ì¬ì •ì˜)
    
    print("ê¸°ì¡´ ë°ì´í„°ì™€ ê°œì„ ëœ ë°ì´í„°ì˜ ì°¨ì´:")
    print("- íŒ¨í„´ ê°„ ê²¹ì¹¨ ì¦ê°€")
    print("- ë…¸ì´ì¦ˆ ë ˆë²¨ ìƒìŠ¹")
    print("- ë” ë‹¤ì–‘í•œ ê³µê²© ê°•ë„")
    
    return None

if __name__ == "__main__":
    diagnose_perfect_performance()
