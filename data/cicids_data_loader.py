# data/cicids_data_loader.py
import pandas as pd
import numpy as np
import requests
import zipfile
import os
from pathlib import Path
import logging
from typing import Tuple
import warnings
warnings.filterwarnings('ignore')

class CICIDSDataLoader:
    """CICIDS2017 ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ë° ì „ì²˜ë¦¬"""
    
    def __init__(self, data_dir: str = "data/cicids2017"):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # CICIDS2017 ë°ì´í„°ì…‹ URL (ê³µì‹)
        self.dataset_urls = {
            "Monday": "https://www.unb.ca/cic/datasets/ids-2017.html",
            # ì‹¤ì œ ë‹¤ìš´ë¡œë“œëŠ” ìˆ˜ë™ìœ¼ë¡œ ì§„í–‰ (ì•½ 2.8GB)
        }
        
        # ì»¬ëŸ¼ ì´ë¦„ ì •ë¦¬ (ê³µë°± ì œê±°)
        self.column_mapping = {
            ' Destination Port': 'Destination_Port',
            ' Flow Duration': 'Flow_Duration',
            ' Total Fwd Packets': 'Total_Fwd_Packets',
            ' Total Backward Packets': 'Total_Backward_Packets',
            ' Label': 'Label'
        }
        
    def download_sample_data(self):
        """ìƒ˜í”Œ ë°ì´í„° ìƒì„± (ì‹¤ì œ CICIDS íŒ¨í„´ ì‹œë®¬ë ˆì´ì…˜)"""
        print("CICIDS2017 ìŠ¤íƒ€ì¼ ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì¤‘...")
        
        np.random.seed(42)
        
        # ì •ìƒ íŠ¸ë˜í”½ íŒ¨í„´ (80%)
        normal_samples = 8000
        normal_data = {
            'Flow_Duration': np.random.exponential(1000000, normal_samples),  # ë§ˆì´í¬ë¡œì´ˆ
            'Total_Fwd_Packets': np.random.poisson(10, normal_samples),
            'Total_Backward_Packets': np.random.poisson(8, normal_samples),
            'Total_Length_Fwd_Packets': np.random.normal(500, 200, normal_samples),
            'Total_Length_Bwd_Packets': np.random.normal(400, 150, normal_samples),
            'Fwd_Packet_Length_Max': np.random.normal(800, 300, normal_samples),
            'Fwd_Packet_Length_Min': np.random.normal(50, 20, normal_samples),
            'Fwd_Packet_Length_Mean': np.random.normal(200, 80, normal_samples),
            'Bwd_Packet_Length_Max': np.random.normal(700, 250, normal_samples),
            'Bwd_Packet_Length_Min': np.random.normal(40, 15, normal_samples),
            'Bwd_Packet_Length_Mean': np.random.normal(180, 70, normal_samples),
            'Flow_Bytes_s': np.random.normal(1000, 500, normal_samples),
            'Flow_Packets_s': np.random.normal(10, 5, normal_samples),
            'Flow_IAT_Mean': np.random.exponential(100000, normal_samples),
            'Flow_IAT_Std': np.random.exponential(50000, normal_samples),
            'Fwd_IAT_Total': np.random.exponential(500000, normal_samples),
            'Fwd_IAT_Mean': np.random.exponential(50000, normal_samples),
            'Bwd_IAT_Total': np.random.exponential(400000, normal_samples),
            'Bwd_IAT_Mean': np.random.exponential(45000, normal_samples),
            'Label': ['BENIGN'] * normal_samples
        }
        
        # DDoS ê³µê²© íŒ¨í„´ (10%)
        ddos_samples = 1000
        ddos_data = {
            'Flow_Duration': np.random.exponential(100000, ddos_samples),  # ì§§ì€ ì§€ì†ì‹œê°„
            'Total_Fwd_Packets': np.random.poisson(100, ddos_samples),    # ë§ì€ íŒ¨í‚·
            'Total_Backward_Packets': np.random.poisson(2, ddos_samples), # ì ì€ ì‘ë‹µ
            'Total_Length_Fwd_Packets': np.random.normal(5000, 1000, ddos_samples),
            'Total_Length_Bwd_Packets': np.random.normal(100, 50, ddos_samples),
            'Fwd_Packet_Length_Max': np.random.normal(1500, 200, ddos_samples),
            'Fwd_Packet_Length_Min': np.random.normal(40, 10, ddos_samples),
            'Fwd_Packet_Length_Mean': np.random.normal(60, 20, ddos_samples),
            'Bwd_Packet_Length_Max': np.random.normal(100, 30, ddos_samples),
            'Bwd_Packet_Length_Min': np.random.normal(20, 5, ddos_samples),
            'Bwd_Packet_Length_Mean': np.random.normal(30, 10, ddos_samples),
            'Flow_Bytes_s': np.random.normal(10000, 3000, ddos_samples),   # ë†’ì€ ë°”ì´íŠ¸ìœ¨
            'Flow_Packets_s': np.random.normal(100, 30, ddos_samples),     # ë†’ì€ íŒ¨í‚·ìœ¨
            'Flow_IAT_Mean': np.random.exponential(1000, ddos_samples),    # ì§§ì€ ê°„ê²©
            'Flow_IAT_Std': np.random.exponential(500, ddos_samples),
            'Fwd_IAT_Total': np.random.exponential(10000, ddos_samples),
            'Fwd_IAT_Mean': np.random.exponential(100, ddos_samples),
            'Bwd_IAT_Total': np.random.exponential(50000, ddos_samples),
            'Bwd_IAT_Mean': np.random.exponential(5000, ddos_samples),
            'Label': ['DDoS'] * ddos_samples
        }
        
        # ì›¹ ê³µê²© íŒ¨í„´ (5%)
        web_attack_samples = 500
        web_attack_data = {
            'Flow_Duration': np.random.exponential(200000, web_attack_samples),
            'Total_Fwd_Packets': np.random.poisson(20, web_attack_samples),
            'Total_Backward_Packets': np.random.poisson(15, web_attack_samples),
            'Total_Length_Fwd_Packets': np.random.normal(2000, 500, web_attack_samples),
            'Total_Length_Bwd_Packets': np.random.normal(800, 200, web_attack_samples),
            'Fwd_Packet_Length_Max': np.random.normal(1200, 300, web_attack_samples),
            'Fwd_Packet_Length_Min': np.random.normal(100, 30, web_attack_samples),
            'Fwd_Packet_Length_Mean': np.random.normal(300, 100, web_attack_samples),
            'Bwd_Packet_Length_Max': np.random.normal(600, 150, web_attack_samples),
            'Bwd_Packet_Length_Min': np.random.normal(50, 15, web_attack_samples),
            'Bwd_Packet_Length_Mean': np.random.normal(150, 50, web_attack_samples),
            'Flow_Bytes_s': np.random.normal(2000, 800, web_attack_samples),
            'Flow_Packets_s': np.random.normal(15, 8, web_attack_samples),
            'Flow_IAT_Mean': np.random.exponential(80000, web_attack_samples),
            'Flow_IAT_Std': np.random.exponential(40000, web_attack_samples),
            'Fwd_IAT_Total': np.random.exponential(300000, web_attack_samples),
            'Fwd_IAT_Mean': np.random.exponential(15000, web_attack_samples),
            'Bwd_IAT_Total': np.random.exponential(200000, web_attack_samples),
            'Bwd_IAT_Mean': np.random.exponential(12000, web_attack_samples),
            'Label': ['Web Attack'] * web_attack_samples
        }
        
        # ë¸Œë£¨íŠ¸í¬ìŠ¤ ê³µê²© íŒ¨í„´ (3%)
        brute_force_samples = 300
        brute_force_data = {
            'Flow_Duration': np.random.exponential(50000, brute_force_samples),
            'Total_Fwd_Packets': np.random.poisson(50, brute_force_samples),
            'Total_Backward_Packets': np.random.poisson(3, brute_force_samples),
            'Total_Length_Fwd_Packets': np.random.normal(1000, 200, brute_force_samples),
            'Total_Length_Bwd_Packets': np.random.normal(200, 50, brute_force_samples),
            'Fwd_Packet_Length_Max': np.random.normal(500, 100, brute_force_samples),
            'Fwd_Packet_Length_Min': np.random.normal(30, 10, brute_force_samples),
            'Fwd_Packet_Length_Mean': np.random.normal(50, 15, brute_force_samples),
            'Bwd_Packet_Length_Max': np.random.normal(200, 50, brute_force_samples),
            'Bwd_Packet_Length_Min': np.random.normal(20, 5, brute_force_samples),
            'Bwd_Packet_Length_Mean': np.random.normal(40, 10, brute_force_samples),
            'Flow_Bytes_s': np.random.normal(5000, 1000, brute_force_samples),
            'Flow_Packets_s': np.random.normal(50, 15, brute_force_samples),
            'Flow_IAT_Mean': np.random.exponential(2000, brute_force_samples),
            'Flow_IAT_Std': np.random.exponential(1000, brute_force_samples),
            'Fwd_IAT_Total': np.random.exponential(20000, brute_force_samples),
            'Fwd_IAT_Mean': np.random.exponential(500, brute_force_samples),
            'Bwd_IAT_Total': np.random.exponential(30000, brute_force_samples),
            'Bwd_IAT_Mean': np.random.exponential(3000, brute_force_samples),
            'Label': ['Brute Force'] * brute_force_samples
        }
        
        # í¬íŠ¸ ìŠ¤ìº” íŒ¨í„´ (2%)
        port_scan_samples = 200
        port_scan_data = {
            'Flow_Duration': np.random.exponential(10000, port_scan_samples),
            'Total_Fwd_Packets': np.random.poisson(5, port_scan_samples),
            'Total_Backward_Packets': np.random.poisson(1, port_scan_samples),
            'Total_Length_Fwd_Packets': np.random.normal(200, 50, port_scan_samples),
            'Total_Length_Bwd_Packets': np.random.normal(50, 20, port_scan_samples),
            'Fwd_Packet_Length_Max': np.random.normal(100, 20, port_scan_samples),
            'Fwd_Packet_Length_Min': np.random.normal(20, 5, port_scan_samples),
            'Fwd_Packet_Length_Mean': np.random.normal(40, 10, port_scan_samples),
            'Bwd_Packet_Length_Max': np.random.normal(80, 15, port_scan_samples),
            'Bwd_Packet_Length_Min': np.random.normal(15, 3, port_scan_samples),
            'Bwd_Packet_Length_Mean': np.random.normal(25, 5, port_scan_samples),
            'Flow_Bytes_s': np.random.normal(500, 100, port_scan_samples),
            'Flow_Packets_s': np.random.normal(20, 5, port_scan_samples),
            'Flow_IAT_Mean': np.random.exponential(5000, port_scan_samples),
            'Flow_IAT_Std': np.random.exponential(2000, port_scan_samples),
            'Fwd_IAT_Total': np.random.exponential(5000, port_scan_samples),
            'Fwd_IAT_Mean': np.random.exponential(1000, port_scan_samples),
            'Bwd_IAT_Total': np.random.exponential(10000, port_scan_samples),
            'Bwd_IAT_Mean': np.random.exponential(8000, port_scan_samples),
            'Label': ['PortScan'] * port_scan_samples
        }
        
        # ëª¨ë“  ë°ì´í„° ê²°í•©
        all_data = {}
        for key in normal_data.keys():
            all_data[key] = (
                list(normal_data[key]) + 
                list(ddos_data[key]) + 
                list(web_attack_data[key]) + 
                list(brute_force_data[key]) + 
                list(port_scan_data[key])
            )
        
        df = pd.DataFrame(all_data)
        
        # ë°ì´í„° ì„ê¸°
        df = df.sample(frac=1, random_state=42).reset_index(drop=True)
        
        # ìŒìˆ˜ ê°’ ì œê±°
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        df[numeric_columns] = df[numeric_columns].abs()
        
        # ë¬´í•œëŒ€ ê°’ ì²˜ë¦¬
        df = df.replace([np.inf, -np.inf], np.nan)
        df = df.fillna(0)
        
        # ì €ì¥
        output_path = self.data_dir / "cicids2017_sample.csv"
        df.to_csv(output_path, index=False)
        
        print(f"ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì™„ë£Œ: {output_path}")
        print(f"ì´ ìƒ˜í”Œ ìˆ˜: {len(df)}")
        print(f"ë¼ë²¨ ë¶„í¬:\n{df['Label'].value_counts()}")
        
        return df
    
    def load_real_cicids_data(self, file_path: str = None) -> pd.DataFrame:
        """ì‹¤ì œ CICIDS2017 ë°ì´í„° ë¡œë“œ"""
        if file_path is None:
            # ì¼ë°˜ì ì¸ CICIDS2017 íŒŒì¼ëª…ë“¤
            possible_files = [
                self.data_dir / "Monday-WorkingHours.pcap_ISCX.csv",
                self.data_dir / "Tuesday-WorkingHours.pcap_ISCX.csv",
                self.data_dir / "Wednesday-workingHours.pcap_ISCX.csv",
                self.data_dir / "Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv",
                self.data_dir / "Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv",
                self.data_dir / "Friday-WorkingHours-Morning.pcap_ISCX.csv",
                self.data_dir / "Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv",
                self.data_dir / "Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv",
            ]
            
            for file_path in possible_files:
                if file_path.exists():
                    break
            else:
                print("ì‹¤ì œ CICIDS2017 íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                print("ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”:")
                print("1. https://www.unb.ca/cic/datasets/ids-2017.html")
                print("2. Kaggle: https://www.kaggle.com/cicdataset/cicids2017")
                return None
        
        try:
            print(f"CICIDS2017 ë°ì´í„° ë¡œë“œ ì¤‘: {file_path}")
            df = pd.read_csv(file_path)
            
            # ì»¬ëŸ¼ëª… ì •ë¦¬
            df.columns = df.columns.str.strip()
            
            # ë¬´í•œëŒ€ ê°’ ì²˜ë¦¬
            df = df.replace([np.inf, -np.inf], np.nan)
            df = df.fillna(0)
            
            print(f"ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)} ìƒ˜í”Œ")
            print(f"ë¼ë²¨ ë¶„í¬:\n{df['Label'].value_counts()}")
            
            return df
            
        except Exception as e:
            print(f"ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}")
            return None
    
    def preprocess_for_api_logs(self, df: pd.DataFrame) -> pd.DataFrame:
        """API ë¡œê·¸ ë¶„ì„ì— ë§ê²Œ ì „ì²˜ë¦¬"""
        
        # API ë¡œê·¸ ê´€ë ¨ íŠ¹ì„±ë§Œ ì„ íƒ
        api_relevant_features = [
            'Flow_Duration',
            'Total_Fwd_Packets', 
            'Total_Backward_Packets',
            'Flow_Bytes_s',
            'Flow_Packets_s', 
            'Flow_IAT_Mean',
            'Fwd_IAT_Mean',
            'Bwd_IAT_Mean',
            'Fwd_Packet_Length_Mean',
            'Bwd_Packet_Length_Mean'
        ]
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
        available_features = [col for col in api_relevant_features if col in df.columns]
        
        if len(available_features) == 0:
            print("ì‚¬ìš© ê°€ëŠ¥í•œ íŠ¹ì„±ì´ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        # íŠ¹ì„± ë°ì´í„°
        X = df[available_features].copy()
        
        # ë¼ë²¨ ì´ì§„í™” (BENIGN = 0, ë‚˜ë¨¸ì§€ = 1)
        y = (df['Label'] != 'BENIGN').astype(int)
        
        # ì´ìƒì¹˜ ì œê±° (IQR ë°©ë²•)
        Q1 = X.quantile(0.25)
        Q3 = X.quantile(0.75)
        IQR = Q3 - Q1
        
        # ê·¹ë‹¨ì ì¸ ì´ìƒì¹˜ë§Œ ì œê±°
        lower_bound = Q1 - 3 * IQR
        upper_bound = Q3 + 3 * IQR
        
        for col in X.columns:
            X.loc[X[col] < lower_bound[col], col] = lower_bound[col]
            X.loc[X[col] > upper_bound[col], col] = upper_bound[col]
        
        # ê²°ê³¼ DataFrame ìƒì„±
        result_df = X.copy()
        result_df['Label'] = y
        result_df['Original_Label'] = df['Label']
        
        print(f"ì „ì²˜ë¦¬ ì™„ë£Œ:")
        print(f"- ì‚¬ìš©ëœ íŠ¹ì„±: {available_features}")
        print(f"- ì •ìƒ ìƒ˜í”Œ: {(y == 0).sum()}")
        print(f"- ê³µê²© ìƒ˜í”Œ: {(y == 1).sum()}")
        
        return result_df

# ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ
def demo_data_loading():
    """ë°ì´í„° ë¡œë”© ë°ëª¨"""
    
    # ë°ì´í„° ë¡œë” ì´ˆê¸°í™”
    loader = CICIDSDataLoader()
    
    # ë°©ë²• 1: ìƒ˜í”Œ ë°ì´í„° ìƒì„± (ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥)
    print("=== ë°©ë²• 1: ìƒ˜í”Œ ë°ì´í„° ìƒì„± ===")
    sample_df = loader.download_sample_data()
    processed_sample = loader.preprocess_for_api_logs(sample_df)
    
    # ë°©ë²• 2: ì‹¤ì œ CICIDS2017 ë°ì´í„° ì‚¬ìš© (ë‹¤ìš´ë¡œë“œ í›„)
    print("\n=== ë°©ë²• 2: ì‹¤ì œ CICIDS2017 ë°ì´í„° ===")
    real_df = loader.load_real_cicids_data()
    if real_df is not None:
        processed_real = loader.preprocess_for_api_logs(real_df)
    
    return processed_sample

# í†µí•© ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
def setup_complete_system():
    """ì™„ì „í•œ ì‹œìŠ¤í…œ ì„¤ì •"""
    from core.anomaly_detection import APILogAnomalyDetector
    
    print("ğŸš€ API ë¡œê·¸ ì´ìƒ íƒì§€ ì‹œìŠ¤í…œ ì„¤ì • ì‹œì‘")
    
    # 1. ë°ì´í„° ì¤€ë¹„
    print("\n1ï¸âƒ£ ë°ì´í„° ì¤€ë¹„...")
    loader = CICIDSDataLoader()
    df = loader.download_sample_data()
    processed_df = loader.preprocess_for_api_logs(df)
    
    # 2. ëª¨ë¸ í›ˆë ¨
    print("\n2ï¸âƒ£ ëª¨ë¸ í›ˆë ¨...")
    detector = APILogAnomalyDetector(model_type='hybrid')
    
    # íŠ¹ì„±ê³¼ ë¼ë²¨ ë¶„ë¦¬
    feature_columns = [col for col in processed_df.columns if col not in ['Label', 'Original_Label']]
    X = processed_df[feature_columns].values
    y = processed_df['Label'].values
    
    # í›ˆë ¨ ì‹¤í–‰
    history = detector.train(X, y, epochs=20)
    
    # 3. ëª¨ë¸ ì €ì¥
    print("\n3ï¸âƒ£ ëª¨ë¸ ì €ì¥...")
    detector.save_model()
    
    # 4. í…ŒìŠ¤íŠ¸
    print("\n4ï¸âƒ£ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸...")
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    test_normal = {
        "timestamp": "2025-07-12T15:30:00",
        "client_ip": "192.168.1.100",
        "method": "POST",
        "url": "/api/v1/customer/segment",
        "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)",
        "requests_per_minute": 3,
        "request_size": 256,
        "content_length": 128,
        "processing_time": 0.12
    }
    
    test_suspicious = {
        "timestamp": "2025-07-12T15:30:00",
        "client_ip": "10.0.0.1",
        "method": "POST",
        "url": "/api/v1/customer/segment?id=1' UNION SELECT * FROM users--",
        "user_agent": "sqlmap/1.4.12#stable",
        "requests_per_minute": 150,
        "request_size": 5000,
        "content_length": 2048,
        "processing_time": 5.0
    }
    
    # ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
    prob_normal, is_anomaly_normal = detector.predict(test_normal)
    prob_suspicious, is_anomaly_suspicious = detector.predict(test_suspicious)
    
    print(f"ì •ìƒ ìš”ì²­ - ì´ìƒ í™•ë¥ : {prob_normal:.3f}, ì´ìƒ ì—¬ë¶€: {is_anomaly_normal}")
    print(f"ì˜ì‹¬ ìš”ì²­ - ì´ìƒ í™•ë¥ : {prob_suspicious:.3f}, ì´ìƒ ì—¬ë¶€: {is_anomaly_suspicious}")
    
    print("\nâœ… ì‹œìŠ¤í…œ ì„¤ì • ì™„ë£Œ!")
    print("\në‹¤ìŒ ë‹¨ê³„:")
    print("1. FastAPI ì„œë²„ ì‹¤í–‰: uvicorn api.customer_api:app --reload")
    print("2. íŠ¸ë˜í”½ ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰")
    print("3. ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ í™•ì¸")
    
    return detector

if __name__ == "__main__":
    # ì‹œìŠ¤í…œ ì „ì²´ ì„¤ì •
    detector = setup_complete_system()
