# íŒŒì¼ëª…: test/integration/improved_memory_test.py

import sys
import os

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
sys.path.insert(0, project_root)

import psutil
import pandas as pd
import numpy as np
import time
import gc
from data.loaders.cicids_working_files_loader import WorkingCICIDSLoader


def get_memory_usage():
    """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë°˜í™˜ (MB)"""
    process = psutil.Process()
    memory_info = process.memory_info()
    return memory_info.rss / 1024 / 1024  # MB ë‹¨ìœ„


def quick_capacity_test():
    """ë¹ ë¥¸ ë©”ëª¨ë¦¬ ìš©ëŸ‰ í…ŒìŠ¤íŠ¸ (ë³´ìˆ˜ì  ì²´í¬ ì™„í™”)"""

    print("ğŸš€ ê°œì„ ëœ ë©”ëª¨ë¦¬ ìš©ëŸ‰ í…ŒìŠ¤íŠ¸")
    print("=" * 50)

    # ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì •ë³´
    total_memory = psutil.virtual_memory().total / 1024 / 1024 / 1024  # GB
    available_memory = psutil.virtual_memory().available / 1024 / 1024 / 1024

    print(f"ğŸ’» ì‹œìŠ¤í…œ ì´ ë©”ëª¨ë¦¬: {total_memory:.1f} GB")
    print(f"ğŸ’» ì‚¬ìš© ê°€ëŠ¥ ë©”ëª¨ë¦¬: {available_memory:.1f} GB")

    # í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜ë“¤ (ë” ê³µê²©ì ìœ¼ë¡œ)
    test_samples = [100000, 200000, 500000, 1000000]

    data_dir = os.path.join(project_root, "data", "cicids2017")
    loader = WorkingCICIDSLoader(data_dir)

    max_successful = 0

    for target_samples in test_samples:
        print(f"\nğŸ” í…ŒìŠ¤íŠ¸: {target_samples:,}ê°œ ìƒ˜í”Œ")

        try:
            start_memory = get_memory_usage()
            start_time = time.time()

            # ë°ì´í„° ë¡œë“œ
            dataset = loader.load_working_files(target_samples=target_samples)

            load_memory = get_memory_usage()
            load_time = time.time() - start_time

            # ì„±ê³µ!
            max_successful = target_samples

            print(f"   âœ… ì„±ê³µ!")
            print(f"   â±ï¸ ë¡œë”© ì‹œê°„: {load_time:.1f}ì´ˆ")
            print(f"   ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©: {load_memory:.1f} MB")
            print(f"   ğŸ’¾ ë©”ëª¨ë¦¬ ì¦ê°€: {load_memory - start_memory:.1f} MB")

            # ë¼ë²¨ ë¶„í¬ í™•ì¸
            label_dist = dataset['Label'].value_counts()
            print(f"   ğŸ·ï¸ ë¼ë²¨ ë¶„í¬:")
            for label, count in label_dist.head().items():
                pct = (count / len(dataset)) * 100
                print(f"      - {label}: {count:,}ê°œ ({pct:.1f}%)")

            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del dataset
            gc.collect()
            time.sleep(1)

            # ì™„í™”ëœ ì•ˆì „ ì²´í¬ (90% ì´ìƒì—ì„œë§Œ ì¤‘ë‹¨)
            current_usage = psutil.virtual_memory().percent
            print(f"   ğŸ“Š ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : {current_usage:.1f}%")

            if current_usage > 90:
                print(f"   âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  {current_usage:.1f}% - ì¤‘ë‹¨")
                break

        except MemoryError:
            print(f"   âŒ ë©”ëª¨ë¦¬ ë¶€ì¡±! ìµœëŒ€ ì²˜ë¦¬ëŸ‰: {max_successful:,}ê°œ")
            break

        except Exception as e:
            print(f"   âŒ ì˜¤ë¥˜: {str(e)[:100]}")
            break

    return max_successful


def test_balanced_loading():
    """ê· í˜•ì¡íŒ ë°ì´í„° ë¡œë”© í…ŒìŠ¤íŠ¸"""
    print("\nğŸ¯ ê· í˜•ì¡íŒ ë°ì´í„° ë¡œë”© í…ŒìŠ¤íŠ¸")
    print("=" * 40)

    data_dir = os.path.join(project_root, "data", "cicids2017")
    loader = WorkingCICIDSLoader(data_dir)

    # 20ë§Œ ê°œë¡œ í…ŒìŠ¤íŠ¸ (4ê°œ íŒŒì¼ ê· ë“± ë¶„ë°°)
    target_samples = 200000

    try:
        start_time = time.time()
        dataset = loader.load_working_files(target_samples=target_samples)
        load_time = time.time() - start_time

        print(f"âœ… {len(dataset):,}ê°œ ë¡œë“œ ì„±ê³µ ({load_time:.1f}ì´ˆ)")

        # ìƒì„¸ ë¼ë²¨ ë¶„í¬
        label_dist = dataset['Label'].value_counts()
        print(f"\nğŸ“Š ìƒì„¸ ë¼ë²¨ ë¶„í¬:")

        attack_count = 0
        for label, count in label_dist.items():
            pct = (count / len(dataset)) * 100
            print(f"   - {label}: {count:,}ê°œ ({pct:.1f}%)")

            if label != 'BENIGN':
                attack_count += count

        attack_ratio = (attack_count / len(dataset)) * 100
        print(f"\nğŸ¯ ì´ ê³µê²© ë¹„ìœ¨: {attack_ratio:.1f}%")

        if attack_ratio > 20:
            print("âœ… ê· í˜•ì¡íŒ ë°ì´í„° í™•ë³´!")
            return dataset
        else:
            print("âš ï¸ ê³µê²© ë°ì´í„° ë¶€ì¡±")
            return None

    except Exception as e:
        print(f"âŒ ê· í˜• í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return None


def estimate_training_capacity(dataset):
    """ì‹¤ì œ ëª¨ë¸ í›ˆë ¨ ìš©ëŸ‰ ì¶”ì •"""
    print("\nğŸ¤– ì‹¤ì œ í›ˆë ¨ ìš©ëŸ‰ ì¶”ì •")
    print("=" * 30)

    if dataset is None:
        return

    # ì „ì²˜ë¦¬
    numeric_columns = dataset.select_dtypes(include=[np.number]).columns
    X = dataset[numeric_columns].fillna(0)

    print(f"ğŸ“Š íŠ¹ì„± ìˆ˜: {len(numeric_columns)}")
    print(f"ğŸ“Š ìƒ˜í”Œ ìˆ˜: {len(X)}")

    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
    data_memory = X.memory_usage(deep=True).sum() / 1024 / 1024
    print(f"ğŸ’¾ ì „ì²˜ë¦¬ ë°ì´í„° í¬ê¸°: {data_memory:.1f} MB")

    # TensorFlow ëª¨ë¸ ë©”ëª¨ë¦¬ ì¶”ì • (ê²½í—˜ì  ê³µì‹)
    # ëŒ€ëµ ë°ì´í„° í¬ê¸°ì˜ 3-5ë°°ê°€ ëª¨ë¸ í›ˆë ¨ ì‹œ í•„ìš”
    estimated_training_memory = data_memory * 4
    print(f"ğŸ’¾ ì˜ˆìƒ í›ˆë ¨ ë©”ëª¨ë¦¬: {estimated_training_memory:.1f} MB")

    # ì‹œìŠ¤í…œ ì—¬ìœ  ë©”ëª¨ë¦¬ì™€ ë¹„êµ
    available_mb = psutil.virtual_memory().available / 1024 / 1024
    print(f"ğŸ’¾ ì‚¬ìš© ê°€ëŠ¥ ë©”ëª¨ë¦¬: {available_mb:.1f} MB")

    if estimated_training_memory < available_mb * 0.7:  # 70% ë§ˆì§„
        print("âœ… í˜„ì¬ ë°ì´í„°ë¡œ ì•ˆì „í•œ í›ˆë ¨ ê°€ëŠ¥!")

        # í™•ì¥ ê°€ëŠ¥ì„± ê³„ì‚°
        max_scale = (available_mb * 0.7) / data_memory
        max_samples = int(len(dataset) * max_scale)
        print(f"ğŸ’¡ ì´ë¡ ì  ìµœëŒ€ ìƒ˜í”Œ ìˆ˜: {max_samples:,}ê°œ")

    else:
        print("âš ï¸ ë°°ì¹˜ ì²˜ë¦¬ ê¶Œì¥")

        # ê¶Œì¥ ë°°ì¹˜ í¬ê¸°
        safe_samples = int(len(dataset) * (available_mb * 0.7) / estimated_training_memory)
        print(f"ğŸ’¡ ê¶Œì¥ ìƒ˜í”Œ ìˆ˜: {safe_samples:,}ê°œ")


if __name__ == "__main__":
    print("ğŸš€ CICIDS2017 ê°œì„ ëœ ë©”ëª¨ë¦¬ ë¶„ì„")
    print("=" * 60)

    # 1. ë¹ ë¥¸ ìš©ëŸ‰ í…ŒìŠ¤íŠ¸
    max_capacity = quick_capacity_test()
    print(f"\nğŸ¯ ìµœëŒ€ ì²˜ë¦¬ ê°€ëŠ¥: {max_capacity:,}ê°œ")

    # 2. ê· í˜•ì¡íŒ ë°ì´í„° í…ŒìŠ¤íŠ¸
    balanced_data = test_balanced_loading()

    # 3. í›ˆë ¨ ìš©ëŸ‰ ì¶”ì •
    estimate_training_capacity(balanced_data)

    # ìµœì¢… ê¶Œì¥ì‚¬í•­
    print("\n" + "=" * 60)
    print("ğŸ¯ ìµœì¢… ê¶Œì¥ì‚¬í•­:")

    if max_capacity >= 500000:
        print("ğŸ”¥ 50ë§Œ ê°œ+ ì²˜ë¦¬ ê°€ëŠ¥ - ëŒ€ìš©ëŸ‰ ì‹¤ì œ ë°ì´í„° í›ˆë ¨ ê¶Œì¥!")
        print("   â†’ CICIDS2017ë¡œ overfitting ë¬¸ì œ í™•ì‹¤íˆ í•´ê²° ê°€ëŠ¥")
    elif max_capacity >= 200000:
        print("âš¡ 20ë§Œ ê°œ+ ì²˜ë¦¬ ê°€ëŠ¥ - ì¤‘ëŒ€ìš©ëŸ‰ ì‹¤ì œ ë°ì´í„° í›ˆë ¨ ê¶Œì¥!")
        print("   â†’ CICIDS2017 í™œìš©ìœ¼ë¡œ ì„±ëŠ¥ ëŒ€í­ ê°œì„  ê¸°ëŒ€")
    else:
        print("ğŸ“Š RealisticSecurityDataGenerator í™•ì¥ ê¶Œì¥")
        print("   â†’ 50ë§Œ ê°œ ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°ë¡œ ëŒ€ì•ˆ ì ‘ê·¼")

    if balanced_data is not None:
        print(f"\nâœ… ê· í˜•ì¡íŒ ì‹¤ì œ ë°ì´í„° {len(balanced_data):,}ê°œ í™•ë³´!")
        print("   â†’ ì¦‰ì‹œ ëª¨ë¸ í›ˆë ¨ ì§„í–‰ ê°€ëŠ¥")